{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "AV335Vuq9lxJ",
        "tPckmZNfitG9",
        "H9KmoAvTMne5"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GavraMG/StructFlow-Colab/blob/main/StructFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß¨ StructFlow Colab Guide   \n",
        "\n",
        "This notebook walks you through generating CellPACK recipes from sequence and simulation data, predicting structures with AlphaFold, and preparing JSON files for 3D modeling.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step 1: Input Protein Sequences  \n",
        "\n",
        "Mount your **Google Drive** and load a CSV file with protein sequences.  \n",
        "\n",
        "üìÅ **Location:** `/MyDrive/CellPACK/Protein_Sequences_Template.csv`  \n",
        "\n",
        "Each row must contain:\n",
        "- `id`: unique identifier  \n",
        "- `sequence`: protein sequence (AAs)\n",
        "\n",
        "This cell will:\n",
        "- Mount Google Drive\n",
        "- Load your sequence CSV\n",
        "- Prepare job folders for each protein\n",
        "- Save working copy into Colab\n",
        "\n",
        "---\n",
        "\n",
        "## üî¨ Step 1.1: Run AlphaFold Prediction (ColabFold)\n",
        "\n",
        "Run **AlphaFold2** predictions for all sequences using ColabFold.  \n",
        "\n",
        "You can customize:\n",
        "- `template_mode`: `\"none\"`, `\"pdb100\"`, or `\"custom\"`\n",
        "- `num_relax`: `0`, `1`, or `5` relaxation steps\n",
        "\n",
        "The model:\n",
        "- Automatically detects if the sequence is a complex\n",
        "- Saves predicted `.pdb` files to `/content/proteins`\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Step 2: Connect Input Files for Recipe Generation  \n",
        "\n",
        "Copy required metadata and simulation files for your specific cell model.\n",
        "\n",
        "üìÅ **Expected Google Drive Folder:** `/MyDrive/CellPACK/CellPACK_Input_Files/`\n",
        "\n",
        "Required files include:\n",
        "- `protein_data.json`\n",
        "- `genes_data.json`\n",
        "- `simulation-1.h5`\n",
        "- `getSeriesData_monomers1.json`\n",
        "- `getSeriesData_complexes1.json`\n",
        "- `WholeCellData.xlsx`\n",
        "- `ingredient_function.csv`\n",
        "- `method_selection.csv`\n",
        "- `compartment_updated.json`\n",
        "- `updated_function_dictionary.json`\n",
        "- `all_dict.json`\n",
        "\n",
        "These will be copied to: `/content/input_files/`\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Step 2.1: Connect CellPACK Data Assets  \n",
        "\n",
        "This step pulls data assets used during 3D model construction.  \n",
        "\n",
        "üìÅ **Drive Source Folder:** `/MyDrive/CellPACK/CellPACK_Data/`\n",
        "\n",
        "Copied folders:\n",
        "- ‚úÖ `lattices/` ‚Äî required for LatticeNucleoid modeling  \n",
        "- üé® `palettes/` ‚Äî optional visualization files  \n",
        "- üíæ `proteins/` ‚Äî your predicted AlphaFold models (auto-copied)\n",
        "\n",
        "Output directory: `/content/cellPACK_Data`\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Step 2.2: Generate CSV Recipes (Auto / Curated)  \n",
        "\n",
        "Choose how you'd like to generate your recipe files:\n",
        "\n",
        "```python\n",
        "recipe_mode = \"both\"  # Options: \"curated\", \"auto\", \"both\"\n",
        "\n",
        "# üîß Definitions:\n",
        "\n",
        "\"curated\" ‚Äì Generates a recipe using manually curated homology models and experimentally validated PDB structures.\n",
        "\n",
        "\"auto\" ‚Äì Includes only proteins with existing solved homologs, without applying any homology modeling.\n",
        "\n",
        "\"both\" ‚Äì Produces both the curated and automated recipe variants for side-by-side use or comparison.\n",
        "\n",
        "---\n",
        "\n",
        "## üåü Step 2.3: Customize Frame Numbers (Copy Number Control)\n",
        "\n",
        "Frame numbers determine which snapshot of the `.h5` simulation is used to calculate molecular copy numbers. This step is essential for producing realistic densities in the final 3D cell model.\n",
        "\n",
        "In the builder script (`WC-MG-CellPACK-RecipeBuilder-short.py`), locate the following lines:\n",
        "\n",
        "- `frame_mono = 149`\n",
        "- `frame_complex = 150`\n",
        "\n",
        "You can modify these values to target different timepoints in the simulation.\n",
        "\n",
        "### üî¢ Example Frame Options:\n",
        "- `frames_m = [150, 1185, 6974]`  ‚Üê for monomers\n",
        "- `frames_c = [146, 1190, 6961]`  ‚Üê for complexes\n",
        "\n",
        "üîÄ Change these values to explore how molecular counts vary across different simulation snapshots.\n",
        "\n",
        "---\n",
        "\n",
        "## üìÑ Step 3: Convert CSV to JSON for CellPACK-GPU\n",
        "\n",
        "Once you have generated `auto_root.csv` or `curated_root.csv`, the next step is to convert it into a CellPACK-compatible JSON recipe.\n",
        "\n",
        "üìÅ These files are saved automatically in your Colab workspace inside the folder:  \n",
        "`scripts_output/`\n",
        "\n",
        "üîó Open the Mesoscope Web Interface:  \n",
        "üëâ https://mesoscope.scripps.edu/beta/#\n",
        "\n",
        "### üì• To convert your CSV:\n",
        "\n",
        "1. Click **\"Load\"** (top left) and upload your `auto_root.csv` or `curated_root.csv` from the `scripts_output` folder.  \n",
        "2. Your ingredients will appear in the **Recipe Table** (bottom panel).  \n",
        "3. Click **\"Save\"** ‚Üí **cellPACK-gpu recipe** to export the `.json` file.\n",
        "\n",
        "üí° This JSON file can now be used with **CellPACK-GPU** or **Simularium** to assemble and visualize your 3D whole-cell model.\n",
        "\n",
        "---\n",
        "\n",
        "## üö´ Step 4: Run CellPACK-GPU Locally (Manual Setup Required)\n",
        "\n",
        "Due to limited access to internal automation scripts, **this step can no longer be executed directly via the Colab pipeline**.\n",
        "\n",
        "Instead, users can manually download and run **CellPACK-GPU** using the official release.\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Download CellPACK-GPU\n",
        "\n",
        "üîó Download it from the official GitHub release page:  \n",
        "üëâ [MycoplasmaGenitalium v1.0 ‚Äì CellPACK-GPU](https://github.com/ccsb-scripps/MycoplasmaGenitalium/releases/tag/v1.0)\n",
        "\n",
        "1. Scroll down to the **Assets** section.\n",
        "2. Download the `cellPACKGPU.zip` file for your operating system.\n",
        "\n",
        "---\n",
        "\n",
        "### üß∞ How to Use It\n",
        "\n",
        "1. **Extract** the ZIP file to a known location on your computer (e.g., Desktop or Documents).\n",
        "2. Inside the extracted folder, locate the file named:\n",
        "   - `cellPACKgpu.exe` (on Windows)\n",
        "3. **Double-click** to launch the application.\n",
        "\n",
        "You‚Äôll see the CellPACK-GPU interface, similar to a Unity-based simulation window. This tool allows you to load your `.json` recipe (from Mesoscope) and simulate spatial packing.\n",
        "\n",
        "---\n",
        "\n",
        "## üë• Contributors\n",
        "\n",
        "**Markus Gavra** and **Evan Bucholski**  \n",
        "Second-year Software Engineering students at the University of Guelph, with a focus on applying computational tools to real-world scientific challenges.\n",
        "\n",
        "\n",
        "### Pushing the boundaries of what's possible‚Äîone cell at a time."
      ],
      "metadata": {
        "id": "AV335Vuq9lxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Step 1: Predicting Protein Structures with AlphaFold"
      ],
      "metadata": {
        "id": "tPckmZNfitG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Input protein sequences from Drive, then hit `Runtime` ‚Üí `Run all`\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import hashlib\n",
        "from sys import version_info\n",
        "\n",
        "python_version = f\"{version_info.major}.{version_info.minor}\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ==== USER INPUT: File inside MyDrive/CellPACK ====\n",
        "csv_filename_input = \"protein_sequences.csv\"  #@param {type:\"string\"}\n",
        "full_csv_path = f\"/content/drive/MyDrive/CellPACK/{csv_filename_input}\"\n",
        "\n",
        "# Check and read file\n",
        "if not os.path.exists(full_csv_path):\n",
        "    raise FileNotFoundError(f\"‚ùå File not found at: {full_csv_path}\")\n",
        "\n",
        "sequences_df = pd.read_csv(full_csv_path)\n",
        "sequences_df = sequences_df.dropna(subset=[\"id\", \"sequence\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(sequences_df)} protein sequences from: {csv_filename_input}\")\n",
        "\n",
        "# Parameters\n",
        "jobname = 'proteins'\n",
        "num_relax = 0  #@param [0, 1, 5] {type:\"raw\"}\n",
        "template_mode = \"none\"  #@param [\"none\", \"pdb100\", \"custom\"]\n",
        "use_amber = num_relax > 0\n",
        "\n",
        "# Handle templates\n",
        "from google.colab import files\n",
        "if template_mode == \"pdb100\":\n",
        "    use_templates = True\n",
        "    custom_template_path = None\n",
        "elif template_mode == \"custom\":\n",
        "    custom_template_path = os.path.join(jobname, \"template\")\n",
        "    os.makedirs(custom_template_path, exist_ok=True)\n",
        "    uploaded_templates = files.upload()\n",
        "    for fn in uploaded_templates.keys():\n",
        "        os.rename(fn, os.path.join(custom_template_path, fn))\n",
        "    use_templates = True\n",
        "else:\n",
        "    custom_template_path = None\n",
        "    use_templates = False\n",
        "\n",
        "# Hash function\n",
        "def add_hash(x, y):\n",
        "    return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]\n",
        "\n",
        "# Save working copy into content\n",
        "output_folder = \"/content\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "working_copy_path = os.path.join(output_folder, \"protein_sequences.csv\")\n",
        "sequences_df.to_csv(working_copy_path, index=False)\n",
        "\n",
        "print(f\"üìÑ Saved working copy to: {working_copy_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l96PvrJb0N57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cc7e37a-5302-43c7-f284-8fbb63ba2031",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Loaded 3 protein sequences from: protein_sequences.csv\n",
            "üìÑ Saved working copy to: /content/protein_sequences.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "%%time\n",
        "import os\n",
        "USE_AMBER = use_amber\n",
        "USE_TEMPLATES = use_templates\n",
        "PYTHON_VERSION = python_version\n",
        "\n",
        "if not os.path.isfile(\"COLABFOLD_READY\"):\n",
        "  print(\"installing colabfold...\")\n",
        "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")\n",
        "  if os.environ.get('TPU_NAME', False) != False:\n",
        "    os.system(\"pip uninstall -y jax jaxlib\")\n",
        "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")\n",
        "  os.system(\"touch COLABFOLD_READY\")\n",
        "\n",
        "if USE_AMBER or USE_TEMPLATES:\n",
        "  if not os.path.isfile(\"CONDA_READY\"):\n",
        "    print(\"installing conda...\")\n",
        "    os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")\n",
        "    os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")\n",
        "    os.system(\"mamba config --set auto_update_conda false\")\n",
        "    os.system(\"touch CONDA_READY\")\n",
        "\n",
        "if USE_TEMPLATES and not os.path.isfile(\"HH_READY\") and USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "  print(\"installing hhsuite and amber...\")\n",
        "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "  os.system(\"touch HH_READY\")\n",
        "  os.system(\"touch AMBER_READY\")\n",
        "else:\n",
        "  if USE_TEMPLATES and not os.path.isfile(\"HH_READY\"):\n",
        "    print(\"installing hhsuite...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")\n",
        "    os.system(\"touch HH_READY\")\n",
        "  if USE_AMBER and not os.path.isfile(\"AMBER_READY\"):\n",
        "    print(\"installing amber...\")\n",
        "    os.system(f\"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer\")\n",
        "    os.system(\"touch AMBER_READY\")"
      ],
      "metadata": {
        "id": "6mmicSJY214P",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22e85026-2419-480f-9d7f-a48eb3f81594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "installing colabfold...\n",
            "CPU times: user 103 ms, sys: 13.8 ms, total: 117 ms\n",
            "Wall time: 35.1 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### MSA options (custom MSA upload, single sequence, pairing mode)\n",
        "msa_mode = \"mmseqs2_uniref_env\" #@param [\"mmseqs2_uniref_env\", \"mmseqs2_uniref\",\"single_sequence\",\"custom\"]\n",
        "pair_mode = \"unpaired_paired\" #@param [\"unpaired_paired\",\"paired\",\"unpaired\"] {type:\"string\"}\n",
        "#@markdown - \"unpaired_paired\" = pair sequences from same species + unpaired MSA, \"unpaired\" = seperate MSA for each chain, \"paired\" - only use paired sequences.\n",
        "\n",
        "# decide which a3m to use\n",
        "if \"mmseqs2\" in msa_mode:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.a3m\")\n",
        "\n",
        "elif msa_mode == \"custom\":\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.custom.a3m\")\n",
        "  if not os.path.isfile(a3m_file):\n",
        "    custom_msa_dict = files.upload()\n",
        "    custom_msa = list(custom_msa_dict.keys())[0]\n",
        "    header = 0\n",
        "    import fileinput\n",
        "    for line in fileinput.FileInput(custom_msa,inplace=1):\n",
        "      if line.startswith(\">\"):\n",
        "         header = header + 1\n",
        "      if not line.rstrip():\n",
        "        continue\n",
        "      if line.startswith(\">\") == False and header == 1:\n",
        "         query_sequence = line.rstrip()\n",
        "      print(line, end='')\n",
        "\n",
        "    os.rename(custom_msa, a3m_file)\n",
        "    queries_path=a3m_file\n",
        "    print(f\"moving {custom_msa} to {a3m_file}\")\n",
        "\n",
        "else:\n",
        "  a3m_file = os.path.join(jobname,f\"{jobname}.single_sequence.a3m\")\n",
        "  with open(a3m_file, \"w\") as text_file:\n",
        "    text_file.write(\">1\\n%s\" % query_sequence)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JwjyHJTQ24wA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### Advanced settings\n",
        "model_type = \"auto\" #@param [\"auto\", \"alphafold2_ptm\", \"alphafold2_multimer_v1\", \"alphafold2_multimer_v2\", \"alphafold2_multimer_v3\", \"deepfold_v1\", \"alphafold2\"]\n",
        "#@markdown - if `auto` selected, will use `alphafold2_ptm` for monomer prediction and `alphafold2_multimer_v3` for complex prediction.\n",
        "#@markdown Any of the mode_types can be used (regardless if input is monomer or complex).\n",
        "num_recycles = \"3\" #@param [\"auto\", \"0\", \"1\", \"3\", \"6\", \"12\", \"24\", \"48\"]\n",
        "#@markdown - if `auto` selected, will use `num_recycles=20` if `model_type=alphafold2_multimer_v3`, else `num_recycles=3` .\n",
        "recycle_early_stop_tolerance = \"auto\" #@param [\"auto\", \"0.0\", \"0.5\", \"1.0\"]\n",
        "#@markdown - if `auto` selected, will use `tol=0.5` if `model_type=alphafold2_multimer_v3` else `tol=0.0`.\n",
        "relax_max_iterations = 200 #@param [0, 200, 2000] {type:\"raw\"}\n",
        "#@markdown - max amber relax iterations, `0` = unlimited (AlphaFold2 default, can take very long)\n",
        "pairing_strategy = \"greedy\" #@param [\"greedy\", \"complete\"] {type:\"string\"}\n",
        "#@markdown - `greedy` = pair any taxonomically matching subsets, `complete` = all sequences have to match in one line.\n",
        "calc_extra_ptm = False #@param {type:\"boolean\"}\n",
        "#@markdown - return pairwise chain iptm/actifptm\n",
        "\n",
        "#@markdown #### Sample settings\n",
        "#@markdown -  enable dropouts and increase number of seeds to sample predictions from uncertainty of the model.\n",
        "#@markdown -  decrease `max_msa` to increase uncertainity\n",
        "max_msa = \"auto\" #@param [\"auto\", \"512:1024\", \"256:512\", \"64:128\", \"32:64\", \"16:32\"]\n",
        "num_seeds = 1 #@param [1,2,4,8,16] {type:\"raw\"}\n",
        "use_dropout = False #@param {type:\"boolean\"}\n",
        "\n",
        "num_recycles = None if num_recycles == \"auto\" else int(num_recycles)\n",
        "recycle_early_stop_tolerance = None if recycle_early_stop_tolerance == \"auto\" else float(recycle_early_stop_tolerance)\n",
        "if max_msa == \"auto\": max_msa = None\n",
        "\n",
        "#@markdown #### Save settings\n",
        "save_all = False #@param {type:\"boolean\"}\n",
        "save_recycles = False #@param {type:\"boolean\"}\n",
        "save_to_google_drive = False #@param {type:\"boolean\"}\n",
        "#@markdown -  if the save_to_google_drive option was selected, the result zip will be uploaded to your Google Drive\n",
        "dpi = 200 #@param {type:\"integer\"}\n",
        "#@markdown - set dpi for image resolution\n",
        "\n",
        "if save_to_google_drive:\n",
        "  from pydrive2.drive import GoogleDrive\n",
        "  from pydrive2.auth import GoogleAuth\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "  print(\"You are logged into Google Drive and are good to go!\")\n",
        "\n",
        "#@markdown Don't forget to hit `Runtime` -> `Run all` after updating the form."
      ],
      "metadata": {
        "cellView": "form",
        "id": "29b8Xffb3DJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üöÄ Run Prediction\n",
        "display_images = True  #@param {type:\"boolean\"}\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from Bio import BiopythonDeprecationWarning\n",
        "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)\n",
        "\n",
        "from colabfold.download import download_alphafold_params, default_data_dir\n",
        "from colabfold.utils import setup_logging\n",
        "from colabfold.batch import get_queries, run, set_model_type\n",
        "from colabfold.colabfold import plot_protein\n",
        "from colabfold.plot import plot_msa_v2\n",
        "\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n",
        "\n",
        "# GPU check\n",
        "try:\n",
        "    K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()\n",
        "    if \"1\" in K80_chk:\n",
        "        print(\"‚ö†Ô∏è WARNING: Found GPU Tesla K80. Limited to sequences < 1000 residues.\")\n",
        "        os.environ.pop(\"TF_FORCE_UNIFIED_MEMORY\", None)\n",
        "        os.environ.pop(\"XLA_PYTHON_CLIENT_MEM_FRACTION\", None)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# === Parameters ===\n",
        "jobname = \"PREDICATING_PROTEINS\"\n",
        "queries_path = \"/content/protein_sequences.csv\"\n",
        "result_dir = jobname\n",
        "os.makedirs(result_dir, exist_ok=True)\n",
        "\n",
        "# === Plot Callbacks ===\n",
        "def input_features_callback(input_features):\n",
        "    if display_images:\n",
        "        plot_msa_v2(input_features)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
        "    model_name, relaxed = mode\n",
        "    if not relaxed and display_images:\n",
        "        fig = plot_protein(protein_obj, Ls=length, dpi=150)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "# === Setup Logging and Download Parameters ===\n",
        "log_filename = os.path.join(result_dir, \"log.txt\")\n",
        "setup_logging(Path(log_filename))\n",
        "\n",
        "print(f\"üìÑ Loading queries from: {queries_path}\")\n",
        "queries, is_complex = get_queries(queries_path)\n",
        "model_type_final = set_model_type(is_complex, \"auto\")\n",
        "\n",
        "download_alphafold_params(model_type_final, Path(\".\"))\n",
        "\n",
        "# === Run Prediction ===\n",
        "print(f\"üöÄ Running AlphaFold prediction for: {jobname}\")\n",
        "results = run(\n",
        "    queries=queries,\n",
        "    result_dir=result_dir,\n",
        "    use_templates=use_templates,\n",
        "    custom_template_path=custom_template_path,\n",
        "    num_relax=num_relax,\n",
        "    msa_mode=\"mmseqs2_uniref_env\",  # or your preferred mode\n",
        "    model_type=model_type_final,\n",
        "    num_models=5,\n",
        "    num_recycles=3,\n",
        "    relax_max_iterations=0,\n",
        "    recycle_early_stop_tolerance=0.5,\n",
        "    num_seeds=1,\n",
        "    use_dropout=False,\n",
        "    model_order=[1, 2, 3, 4, 5],\n",
        "    is_complex=is_complex,\n",
        "    data_dir=Path(\".\"),\n",
        "    keep_existing_results=False,\n",
        "    rank_by=\"auto\",\n",
        "    pair_mode=\"unpaired+paired\",\n",
        "    pairing_strategy=\"greedy\",\n",
        "    stop_at_score=float(100),\n",
        "    prediction_callback=prediction_callback,\n",
        "    dpi=150,\n",
        "    zip_results=False,\n",
        "    save_all=False,\n",
        "    max_msa=None,\n",
        "    use_cluster_profile=False,\n",
        "    input_features_callback=input_features_callback,\n",
        "    save_recycles=False,\n",
        "    user_agent=\"colabfold/google-colab-main\",\n",
        "    calc_extra_ptm=False,\n",
        ")\n",
        "\n",
        "# === Organize Top-Ranked PDB Output Only ===\n",
        "proteins_dir = \"proteins\"\n",
        "os.makedirs(proteins_dir, exist_ok=True)\n",
        "\n",
        "seen_prefixes = set()\n",
        "\n",
        "for file in sorted(os.listdir(result_dir)):\n",
        "    if not file.endswith(\".pdb\"):\n",
        "        continue\n",
        "    if \"_rank_001_\" not in file:\n",
        "        continue  # skip lower-ranked predictions\n",
        "\n",
        "    # Extract the base protein ID (e.g., \"P001\")\n",
        "    base_id = file.split(\"_\")[0]\n",
        "    if base_id in seen_prefixes:\n",
        "        continue  # already saved top model for this protein\n",
        "\n",
        "    seen_prefixes.add(base_id)\n",
        "    src = os.path.join(result_dir, file)\n",
        "    dst = os.path.join(proteins_dir, f\"{base_id}.pdb\")  # clean name\n",
        "    os.rename(src, dst)\n",
        "\n",
        "print(f\"üìÅ Top-ranked PDB files moved to: {proteins_dir}\")\n",
        "print(\"‚úÖ Only rank_001 structures saved.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SKQGfoG23HEx",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Building the Cell Recipe with CellPACK\n"
      ],
      "metadata": {
        "id": "H9KmoAvTMne5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connecting and Import CellPACK Input_Files from Google Drive (Press `Runtime` ‚Üí `Run all`)\n",
        "\n",
        "from google.colab import drive\n",
        "import os, shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "drive_base = \"/content/drive/MyDrive/CellPACK/CellPACK_Input_Files\"\n",
        "local_base = \"/content/input_files\"\n",
        "os.makedirs(local_base, exist_ok=True)\n",
        "\n",
        "# Required and optional files\n",
        "required_files = [\n",
        "    \"protein_data.json\",\n",
        "    \"genes_data.json\",\n",
        "    \"simulation-1.h5\",\n",
        "    \"getSeriesData_monomers1.json\",\n",
        "    \"getSeriesData_complexes1.json\",\n",
        "    \"WholeCellData.xlsx\",\n",
        "    \"ingredient_function.csv\",\n",
        "    \"method_selection.csv\",\n",
        "    \"compartment_updated.json\",\n",
        "    \"updated_function_dictionary.json\",\n",
        "    \"all_dict.json\"\n",
        "]\n",
        "\n",
        "optional_files = [\n",
        "    \"HHpred_scores.csv\",\n",
        "    \"S3K-Transcription units.csv\",\n",
        "    \"uniprot.csv\"\n",
        "]\n",
        "\n",
        "missing_required_files = []\n",
        "\n",
        "print(\"üìÅ Copying required files from Google Drive...\")\n",
        "for fname in required_files + optional_files:\n",
        "    src = os.path.join(drive_base, fname)\n",
        "    dst = os.path.join(local_base, fname)\n",
        "\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"‚úÖ Copied: {fname}\")\n",
        "    elif fname in required_files:\n",
        "        missing_required_files.append(fname)\n",
        "        print(f\"‚ùå Missing: {fname}\")\n",
        "\n",
        "# Final message\n",
        "if missing_required_files:\n",
        "    print(\"\\n‚ö†Ô∏è The following required files were NOT found in your Google Drive folder:\")\n",
        "    for f in missing_required_files:\n",
        "        print(f\" - {f}\")\n",
        "else:\n",
        "    print(\"\\nüéâ All required files successfully copied to /content/input_files\")\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iwr4VLWLYlxD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd04a4fd-3e2e-4c50-aca4-c5696b54eafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "üìÅ Copying required files from Google Drive...\n",
            "‚úÖ Copied: protein_data.json\n",
            "‚úÖ Copied: genes_data.json\n",
            "‚úÖ Copied: simulation-1.h5\n",
            "‚úÖ Copied: getSeriesData_monomers1.json\n",
            "‚úÖ Copied: getSeriesData_complexes1.json\n",
            "‚úÖ Copied: WholeCellData.xlsx\n",
            "‚úÖ Copied: ingredient_function.csv\n",
            "‚úÖ Copied: method_selection.csv\n",
            "‚úÖ Copied: compartment_updated.json\n",
            "‚úÖ Copied: updated_function_dictionary.json\n",
            "‚úÖ Copied: all_dict.json\n",
            "‚úÖ Copied: HHpred_scores.csv\n",
            "‚úÖ Copied: S3K-Transcription units.csv\n",
            "‚úÖ Copied: uniprot.csv\n",
            "\n",
            "üéâ All required files successfully copied to /content/input_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Connecting and Import CellPACK DATA from Google Drive (Press `Runtime` ‚Üí `Run all`)\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Source: where your folders are in Drive\n",
        "drive_base = \"/content/drive/MyDrive/CellPACK/CellPACK_Data\"\n",
        "\n",
        "# Target: destination in Colab\n",
        "colab_target = \"/content/cellPACK_Data\"\n",
        "os.makedirs(colab_target, exist_ok=True)\n",
        "\n",
        "# Define which folders to copy\n",
        "required_folders = [\"lattices\"]\n",
        "optional_folders = [\"palettes\"]\n",
        "missing = []\n",
        "\n",
        "# Copy folders from Google Drive\n",
        "for folder in required_folders + optional_folders:\n",
        "    src = os.path.join(drive_base, folder)\n",
        "    dst = os.path.join(colab_target, folder)\n",
        "\n",
        "    if os.path.isdir(src):\n",
        "        if os.path.exists(dst):\n",
        "            shutil.rmtree(dst)\n",
        "        shutil.copytree(src, dst)\n",
        "        print(f\"‚úÖ Copied '{folder}' from Google Drive to /content/cellPACK_Data\")\n",
        "    elif folder in required_folders:\n",
        "        missing.append(folder)\n",
        "        print(f\"‚ùå MISSING in Drive: '{folder}'\")\n",
        "\n",
        "# Add Proteins folder from local Colab\n",
        "local_protein_folder = \"/content/proteins\"\n",
        "target_protein_folder = os.path.join(colab_target, \"proteins\")\n",
        "\n",
        "if os.path.isdir(local_protein_folder):\n",
        "    if os.path.exists(target_protein_folder):\n",
        "        shutil.rmtree(target_protein_folder)\n",
        "    shutil.copytree(local_protein_folder, target_protein_folder)\n",
        "    print(\"‚úÖ Added local 'proteins' folder to /content/cellPACK_Data\")\n",
        "else:\n",
        "    print(\"‚ùå 'proteins' folder not found in /content\")\n",
        "\n",
        "# Final report\n",
        "if missing:\n",
        "    print(\"\\n‚ö†Ô∏è Missing required folder(s):\", \", \".join(missing))\n",
        "else:\n",
        "    print(\"\\nüéâ All required folders copied successfully to /content/cellPACK_Data\")\n"
      ],
      "metadata": {
        "id": "bRYwdiJwajFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "279af92a-9ce4-4c7c-fa50-a2663436e286",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Copied 'lattices' from Google Drive to /content/cellPACK_Data\n",
            "‚úÖ Copied 'palettes' from Google Drive to /content/cellPACK_Data\n",
            "‚úÖ Added local 'proteins' folder to /content/cellPACK_Data\n",
            "\n",
            "üéâ All required folders copied successfully to /content/cellPACK_Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import CellPACK Functions script for use\n",
        "\n",
        "\n",
        "%%writefile WC-MG-CellPACK-functions.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Jul  8 13:14:28 2021\n",
        "\n",
        "@author: MM\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import h5py\n",
        "import numpy as np\n",
        "import csv\n",
        "import shutil\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Jul  8 13:14:28 2021\n",
        "\n",
        "@author: MM\n",
        "\"\"\"\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import h5py\n",
        "import numpy as np\n",
        "import csv\n",
        "import shutil\n",
        "import h5py\n",
        "from openpyxl import load_workbook\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# FUNCTIONS TO GET DATA FROM WC-SIMULATIONS and ASSEMBLE A DRAFT RECIPE REDABLE ON MESOSCOPE\n",
        "#----------------------------------------------------------------------------\n",
        "#INPUT FILES INFO\n",
        "    #to run all the following code you need:\n",
        "    # 1) protein database and gene database from cover wholecellkb: 'protein_data.json' | 'genes_data.json'\n",
        "    # 2) simulation file .h5 format: simulation-1.h5 (downloaded from http://www.wholecellsimdb.org/simulation_batch/1))\n",
        "    # 3) json file with positions of DNA binding monomers/complexes: getSeriesData_monomers1.json | getSeriesData_complex1.json\n",
        "    # downloaded from: http://www.wholecellviz.org/getSeriesData.php?sim_id=2011_10_19_02_53_45_1&class_name=Chromosome&attr_name=monomerBoundSites\n",
        "    # and http://www.wholecellviz.org/getSeriesData.php?sim_id=2011_10_19_02_53_45_1&class_name=Chromosome&attr_name=complexBoundSites\n",
        "    # 4) frame number (note that when extracting data from getSeriesData_monomers1.json | getSeriesData_complex1.json or sim\n",
        "    #the frame number is different due to the file type but they refer to the same frame in the simulation.\n",
        "    #Complexes and monomers do not have the same time stamps because\n",
        "\n",
        "#frames json: 150(mono)/146(complex); 1185(mono)/1190 (complex); 6974 (mono)/6961(complex)\n",
        "#frames when working with the hdf5 files is: (frame json) -1\n",
        "#frames sim: 149(mono)/145(complex); 1184(mono)/1189(complex); 6973(mono)/6960(complex)\n",
        "\n",
        "#input files:\n",
        "input_dir ='input_files'+os.sep\n",
        "\n",
        "#outfput folder:\n",
        "output_dir = output_folder = 'scripts_output'+os.sep\n",
        "\n",
        "#holds DNA binding prot positions for monomers\n",
        "fname_m = input_dir+'getSeriesData_monomers1.json'\n",
        "#holds DNA binding prot positions for complexes\n",
        "fname_c = input_dir+'getSeriesData_complexes1.json'\n",
        "#simulation #1-1\n",
        "sim = input_dir+'simulation-1.h5'\n",
        "\n",
        "#TRANSCRIPTION UNIT\n",
        "    #creates a dictionary where every monomer is associated with its transcription unit, gene length, coords, etc\n",
        "    #attention: the function uses some actual file and refers to actual file locations (ie csvf, input_dir, 'genes_data.json')\n",
        "\n",
        "    #you need:\n",
        "    #       hdf5 file\n",
        "    #       beware of 'S3K-Transcription units.csv'\n",
        "    #       'protein_data.json' downloaded from http://www.wholecellkb.org/\n",
        "    #       'genes_data.json' downloaded from http://www.wholecellkb.org/\n",
        "\n",
        "def prepare_dictionary_index(sim):\n",
        "    x = h5py.File(sim)\n",
        "    #create the variable for the labels\n",
        "    labels_mono= x.get(\"states/ProteinMonomer/counts/labels/0\")\n",
        "    #make a numpy array with the monomer labels\n",
        "    array_mono = np.array(labels_mono)\n",
        "    #make initial dictionary index to protein dictionary and to gene ie 482: {'protID': 'MG_470_MONOMER'}\n",
        "    index_dict = {}\n",
        "    for i in array_mono:\n",
        "        if '-nascent'.encode() in i:\n",
        "            index = np.where(array_mono == i)[0][0]+1\n",
        "            #nb. i use decode because of p3 issues with ascii\n",
        "            index_dict[index]={'protID':i.decode().split('-')[0]}\n",
        "    #make a ditionary for trancription units ie. 'TU_335': {'type': 'polycistronic', 'length_TU': 2766, 'start_tu': 577268}\n",
        "    tu = {}\n",
        "    #input is a csv file\n",
        "    csvf = input_dir+'S3K-Transcription units.csv'\n",
        "    with open(csvf) as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        header = next(csvreader)\n",
        "        header = next(csvreader)\n",
        "        header = next(csvreader)\n",
        "        for row in csvreader:\n",
        "            TU = row[0] #TU column\n",
        "            gen = row[3] #genes column\n",
        "            leng = row[5] #length genes column\n",
        "            start_coord = row[4] #start coordinate column\n",
        "            product = row[2]\n",
        "            if 'mRNA' in row[2]:\n",
        "                if ',' in gen: #',' means there are multiple genes in the same TU -> polycistronic\n",
        "                    tu[TU]={'type':'polycistronic', 'length_TU':int(leng)+1, 'start_tu':int(start_coord), 'product':product}\n",
        "                else: #no ',' means a single gene in the TU -> monocistronic\n",
        "                    tu[TU]={'type':'monocistronic', 'length_TU':int(leng)+1, 'start_tu':int(start_coord), 'product':product}\n",
        "            else:\n",
        "                if ',' in gen:\n",
        "                    tu[TU]={'type':'polycistronic', 'length_TU':int(leng)+1, 'start_tu':int(start_coord), 'product':product}\n",
        "                else:\n",
        "                    tu[TU]={'type':'monocistronic', 'length_TU':int(leng)+1, 'start_tu':int(start_coord), 'product':product}\n",
        "    genes = json.load(open(input_dir+'genes_data.json', 'r'))\n",
        "    #make initial dictionary index for genes\n",
        "    gene_dic={}\n",
        "    for agene in genes['data']:\n",
        "        gene_dic[agene['wid']]={'TU':agene['transcription_units'][0], 'gene_length':agene['length'], 'type':agene['type'][0], 'length_TU':''}\n",
        "    for i in gene_dic:\n",
        "        if gene_dic[i]['TU'] in tu:\n",
        "            gene_dic[i].update({'length_TU':tu[gene_dic[i]['TU']]['length_TU']})\n",
        "    #update the index_dict to include tu, gene len, coord, direction, type etc\n",
        "    #ie. 482: {'protID': 'MG_470_MONOMER', 'gene': 'MG_470', 'gene_length': 810, 'TU': 'TU_335',  'coord_gene': 579224,\n",
        "    #          'direction': 'Reverse', 'type': 'polycistronic', 'length_TU': 2766, 'coord_tu': 577268}\n",
        "    covert = json.load(open(input_dir+'protein_data.json', 'r'))\n",
        "    for i in index_dict:\n",
        "        name = index_dict[i]['protID']\n",
        "        for aname in covert['data']:\n",
        "            if name == aname['wid']:\n",
        "                index_dict[i].update({'gene':aname['gene']})\n",
        "                break\n",
        "        gene = index_dict[i]['gene']\n",
        "        for agene in genes['data']:\n",
        "            if gene == agene['wid']:\n",
        "                index_dict[i].update({'gene_length':int(agene['length']), 'TU':agene['transcription_units'][0], 'coord_gene':int(agene['coordinate']), 'direction':agene['direction']})\n",
        "        unit = index_dict[i]['TU']\n",
        "        for u in tu:\n",
        "            if unit == u:\n",
        "                index_dict[i].update({'type':tu[u]['type'],'length_TU':tu[u]['length_TU'], 'coord_tu':tu[u]['start_tu']})\n",
        "    return index_dict, gene_dic\n",
        "\n",
        "#usage\n",
        "index_dict, genes_dict =prepare_dictionary_index(sim)\n",
        "\n",
        "\n",
        "def dna_binding_list(json_data):\n",
        "    covert = json.load(open(input_dir+json_data, 'r'))\n",
        "    l =[]\n",
        "    d={}\n",
        "    for i in covert['data']:\n",
        "        if i['model']=='ProteinMonomer':\n",
        "            if i['dna_footprint']!= None :\n",
        "            #print i['dna_footprint']\n",
        "                l.append(i['wid'])\n",
        "                d[i['wid']]={'dna_footprint':i['dna_footprint']['length']}\n",
        "        if i['model']=='ProteinComplex':\n",
        "            if i['dna_footprint']!= None :\n",
        "           #print i['dna_footprint']\n",
        "               l.append(i['wid'])\n",
        "               d[i['wid']]={'dna_footprint':i['dna_footprint']['length']}\n",
        "    return l, d\n",
        "##usage\n",
        "##this is a list of all DNA binding proteins in the simulation\n",
        "dna_bp_list, dna_bp_dictionary = dna_binding_list('protein_data.json')\n",
        "\n",
        "#PROTEIN COORD CHROMOSOME\n",
        "    #makes a csv file with the proteinid+position+strand from json file\n",
        "    #NB if you want this to work, write the PRECISE frame number found in the json file\n",
        "    #this is based on json so the frame you use here corresponds to -1 in the sim\n",
        "\n",
        "    #you need: hdf5 file (sim)\n",
        "    #         json file for monomer bound to dna (getSeriesData_monomers1.json)\n",
        "    #         json file for complexes bound to dna (getSeriesData_complexes1.json)\n",
        "    #         frame number for monomers\n",
        "    #         frame number for complexes\n",
        "\n",
        "def chromosome_proteins(fname_m, frame_number_m, fname_c, frame_number_c,sim):\n",
        "    z = []\n",
        "    csv = open(output_folder+'chromosome_proteins_'+str(frame_number_m-1)+'.csv', 'w')\n",
        "    csv.write('#DNA_BINDING PROTEINS\\n')\n",
        "    csv.write('MONOMERS FRAME '+' '+str(frame_number_m-1)+'\\n')\n",
        "    csv.write('COMPLEXES FRAME '+' '+str(frame_number_c-1)+'\\n')\n",
        "    csv.write('protId, coordinates, strand, dna_footprint\\n')\n",
        "    #open json file with list of dictionaries: each dictionary represents a DNA-bound protein molecule at an instance of simulation time\n",
        "    mono = json.load(open(fname_m, 'r'))\n",
        "    compl = json.load(open(fname_c, 'r'))\n",
        "    #open the simulation file format hdf5\n",
        "    x = h5py.File(sim)\n",
        "    #array for the monomers nasme <> identification number\n",
        "    labels_mono= x.get(\"states/ProteinMonomer/counts/labels/0\")\n",
        "    labels_compl = x.get(\"states/ProteinComplex/counts/labels/0\")\n",
        "    #make a list with the frame number\n",
        "    frame_m = [frame_number_m]\n",
        "    frame_c= [frame_number_c]\n",
        "    #for a specific frame look into the json file and find:\n",
        "    for i in range(len(mono)):\n",
        "        if mono[i]['time']==frame_m:\n",
        "            for el in range(len(mono[i]['pos'])):\n",
        "                index = mono[i]['val'][el]-1 #-1 BECAUSE IT COMES FROM MATLAB\n",
        "                pos = str(mono[i]['pos'][el])\n",
        "                strand = str(mono[i]['strand'][el])\n",
        "                #print index, labels_mono[index], pos, strand\n",
        "                #print mono[i]['val'][el]-1, labels_mono[index], mono[i]['pos'][el], mono[i]['strand'][el]\n",
        "                protid=labels_mono[index].decode().split('-nascent')[0]\n",
        "                footprint= dna_bp_dictionary[protid]['dna_footprint']\n",
        "                csv.write(protid+','+pos+','+strand+','+footprint+'\\n')\n",
        "    for i in range(len(compl)):\n",
        "        if compl[i]['time']==frame_c:\n",
        "            for el in range(len(compl[i]['pos'])):\n",
        "                index = compl[i]['val'][el]-1  #-1 BECAUSE IT COMES FROM MATLAB\n",
        "                pos = str(compl[i]['pos'][el])\n",
        "                strand = str(compl[i]['strand'][el])\n",
        "                protid=labels_compl[index].decode().split('-nascent')[0]\n",
        "                footprint= dna_bp_dictionary[protid]['dna_footprint']\n",
        "                #print compl[i]['val'][el], labels_compl[index], compl[i]['pos'][el], compl[i]['strand'][el]\n",
        "                #print labels_compl[index], pos, strand\n",
        "                if 'RNA_POLYMERASE' not in labels_compl[index].decode():\n",
        "                    csv.write(labels_compl[index].decode().split('-nascent')[0]+','+pos+','+strand+','+footprint+'\\n')\n",
        "                if 'RNA_POLYMERASE' in labels_compl[index].decode():\n",
        "                    z.append(labels_compl[index])\n",
        "                    pos = str(compl[i]['pos'][el]+37) #+37 because it is half of the rnapoly foot print of 75. It was encoded differently in the h5 and json files so adding +37 gives the numbers of the h5\n",
        "                    csv.write(labels_compl[index].decode().split('-nascent')[0]+','+pos+','+strand+','+footprint+'\\n')\n",
        "            print('rna polymerases number', len(z))\n",
        "    csv.close()\n",
        "\n",
        "#RNA POLYMERASE\n",
        "    #to run this use the number of the json -1\n",
        "    #make a csv file starting from the simulation file and listing the RNApoly count\n",
        "    #in the state column the actively transcribing polymerases are indicated by any number different from 0, -1, -2, -3\n",
        "\n",
        "    #you need: hdf5 file (sim)\n",
        "    #          frame\n",
        "    #          outputfolder\n",
        "\n",
        "def rna_poly(sim, frame):\n",
        "    x = h5py.File(sim)\n",
        "    active = x.get(\"states/RNAPolymerase/nActive/data\")\n",
        "    free = x.get(\"states/RNAPolymerase/nFree/data\")\n",
        "    promoter = x.get(\"states/RNAPolymerase/nSpecificallyBound/data\")\n",
        "    nonspecificallybound = x.get(\"states/RNAPolymerase/nNonSpecificallyBound/data\")\n",
        "    #define the total number of rna_poly\n",
        "    tot_rna_poly = active[0,0,frame]+free[0,0,frame]+promoter[0,0,frame]+nonspecificallybound[0,0,frame]\n",
        "    #print (active[0,0,frame], free[0,0,frame], promoter[0,0,frame], nonspecificallybound[0,0,frame], (tot_rna_poly))\n",
        "    #position and strand\n",
        "    pos_data = x.get(\"states/RNAPolymerase/positionStrands/data\")\n",
        "    #transcribing/ free / non specifically bound / promoter bound. ie. state_data[0][0][500]=state_data[protid][state][frame]\n",
        "    state_data = x.get(\"states/RNAPolymerase/states/data\")\n",
        "    #sotre the length of transcript\n",
        "    TranscriptProgress_data = x.get(\"states/Transcript/boundTranscriptProgress/data\")\n",
        "    #stores the transcription unit index that is being transcribed.\n",
        "    tu_data = x.get(\"states/Transcript/boundTranscriptionUnits/data\")\n",
        "    #make an array of 5 columns with integers full of only 0\n",
        "    #a = np.zeros(shape=(pos_data.shape[0], 5), dtype=np.int)\n",
        "    a = np.zeros(shape=(tot_rna_poly, 5), dtype=np.int)\n",
        "    #update the array witht the two columns of coordinate and strand (pos_data)\n",
        "    a[:,:2] = pos_data[:tot_rna_poly,:,frame]\n",
        "    #fill the 3rd column with state_data. NB (-3=>promoter bound, -2=>free, -1=>non-specifically bound, 0=>not exist, 1=>actively transcribing)\n",
        "    a[:,2] = np.array(state_data[:tot_rna_poly,:,frame]).flatten()\n",
        "    #fill the 4th column with length of transcript TranscriptProgress_data\n",
        "    a[:,3] = np.array(TranscriptProgress_data[:tot_rna_poly,:,frame]).flatten()\n",
        "    #fill the 5th column with the transcription unit index (0 to 334) tu_data\n",
        "    a[:,4] = np.array(tu_data[:tot_rna_poly,:,frame]).flatten()\n",
        "    #write a csv file with this array\n",
        "    np.savetxt(output_folder+'RNApoly_'+str(frame)+'.csv', a,\n",
        "               delimiter=\",\", fmt=\"%d\",\n",
        "               header='coordinate (nt), strand, state, transcript length (nt), TranscriptionUnit id',\n",
        "               comments='#coordinate: refers to the START of RNApoly\\n #state: -3=>promoter bound; -2=>free; -1=>non-specifically bound; 0=>not exist; 1=>actively transcribing\\n #TU not for mRNA=> TU_088: rRNA; TU_178/TU_181/TU_226/TU_227: sRNA;TU_007/TU_037/TU_123/TU_134/TU_135/TU_136/TU_137/TU_138/TU_139/TU_140/TU_141/TU_146/TU_166/TU_168/TU_172/TU_187/TU_188/TU_189/TU_194/TU_220/TU_221/TU_223/TU_248/TU_250/TU_252/TU_276: tRNA\\n ')\n",
        "    #comments='in state: -3=>promoter bound, -2=>free, -1=>non-specifically bound, 0=>not exist, everything =or> 1 =>actively transcribing')\n",
        "#    #this below is just to count the polymerases\n",
        "#    poly = []\n",
        "#    promoter = []\n",
        "#    free = []\n",
        "#    nonspecific = []\n",
        "#    active = []\n",
        "#    for i in pos_data[:, :, frame]:\n",
        "#        coordinate = i[0]\n",
        "#        strand = i[1]\n",
        "#        if strand !=0:\n",
        "#            poly.append(strand)\n",
        "#            #print 'RNA-POLYMERASE', coordinate, strand\n",
        "#            #csv.write('RNA_POLYMERASE'+','+str(coordinate)+','+str(strand)+'\\n')\n",
        "#    for state in state_data[:, :, frame]:\n",
        "#       if state[0]==[-1]:\n",
        "#           nonspecific.append(state[0])\n",
        "#       if state[0]==[-3]:\n",
        "#           promoter.append(state[0])\n",
        "#       if state[0]==[-2]:\n",
        "#           free.append(state[0])\n",
        "#       #every state that is not 0, -1, -2, -3 is ACTIVELY TRANSCRIBING\n",
        "#       if state[0]!=[0] and state[0]!=[-2] and state[0]!=[-1] and state[0]!=[-3]:\n",
        "#           active.append(state[0])\n",
        "    print('tot polymerases number:', (tot_rna_poly))\n",
        "    print ('active', (active[0,0,frame]), 'free', (free[0,0,frame]), 'promoter', (promoter[0,0,frame]), 'Non specifically bound', (nonspecificallybound[0,0,frame]))\n",
        "#    print('actively transcribing (1 or 0): ', len(active))\n",
        "#    print('free (-2 or 3): ', len(free))\n",
        "#    print('non specific bound(-1 or 2) :', len(nonspecific))\n",
        "#    print('promoter bound (-3 or 1): ', len(promoter))\n",
        "\n",
        "#RNAs\n",
        "    #output 1 (now commented):csv wit a list of sRNA and mRNA\n",
        "    #output 2:arrays of mRNA, tRNA, sRNA, rRNA\n",
        "    #TOTAL NUMBER OF RNAS, NOT FREE RNAS\n",
        "\n",
        "def free_rna_count(sim, frame):\n",
        "    d, genes_dict =prepare_dictionary_index(sim)\n",
        "    f = h5py.File(sim)\n",
        "    labels0 = f.get(\"states/Rna/counts/labels/0\")\n",
        "    labels_tu = np.array(labels0)\n",
        "    data_rna = f.get(\"states/Rna/counts/data\")\n",
        "    #array with only compartment 0 at a specific frame\n",
        "    z =data_rna[frame, 0]\n",
        "        #data for the non zero RNA molecules\n",
        "    non_zero =z[z>0]\n",
        "        #find the id for the RNA molecules > 0 and make and array\n",
        "    idx_non_zero = np.where(data_rna[frame, 0]>0) #this is a tuple\n",
        "    idx_RNA = np.array(idx_non_zero)\n",
        "        #make a new array with zero\n",
        "    a=np.zeros(non_zero.shape,\n",
        "                   dtype = 'i4, U20, i4, i4, U20')\n",
        "        #column 0: RNA ids\n",
        "    a['f0'] = np.array(idx_RNA[:]).flatten()\n",
        "        #column 2: count of RNA molecules\n",
        "    a['f2'] = np.array(non_zero[:]).flatten()\n",
        "    for i,idx in enumerate(a['f0']):\n",
        "        #column 1: name of RNA molecule\n",
        "        a['f1'][i]= labels_tu[idx]\n",
        "        rna_id =labels_tu[idx].decode().split('-')[0]\n",
        "        #print (rna_id)\n",
        "        if rna_id in genes_dict:#for rRNA, tRNA, sRNA\n",
        "            #column 3: gene len\n",
        "            #column 4: RNA type\n",
        "            a['f3'][i]= genes_dict[rna_id]['gene_length']\n",
        "            a['f4'][i]= genes_dict[rna_id]['type']\n",
        "        if rna_id not in genes_dict:  #mRNA\n",
        "            #print (rna_id)\n",
        "            for x in genes_dict:\n",
        "                if rna_id==genes_dict[x]['TU']:\n",
        "                    a['f3'][i]= genes_dict[x]['length_TU']\n",
        "                    a['f4'][i]= genes_dict[x]['type'] #reorder the array on mRNA, tRNA, rRNA, sRNA column\n",
        "        #a_sorted = np.sort(a, order=['f3'], axis=0)\n",
        "        #depending on what you want you can create csv with the different types of free RNAs\n",
        "    mRNA = a[a['f4']=='mRNA']\n",
        "    sRNA = a[a['f4']=='sRNA']\n",
        "    rRNA = a[a['f4']=='rRNA']\n",
        "    tRNA_1 = a[a['f4']=='tRNA'] #careful here because there are also initiator_tRNA\n",
        "    tRNA_2 = a[a['f4']=='initiator_tRNA']\n",
        "    tRNA = np.concatenate((tRNA_1, tRNA_2))\n",
        "    arr = np.concatenate((sRNA, mRNA))\n",
        "    arr_ordered = np.sort(arr, order=['f4'])\n",
        "    return tRNA, rRNA, sRNA, mRNA\n",
        "\n",
        "#RIBOSOME\n",
        "    #to run this use the number of the json -1\n",
        "    #make a csv file starting from the simulation file and listing the ribosomes position, directions, states etc\n",
        "    #you need:\n",
        "    #       hdf5 file (sim)\n",
        "    #       frame\n",
        "    #outputfolder\n",
        "    #run ribosome\n",
        "    #run prepare_dictionary_index(sim)\n",
        "\n",
        "#(PART 1)\n",
        "    #make an array with ribosome positions, SHIFTED positons, directions, states etc\n",
        "def ribosome(sim, frame, output_folder):\n",
        "    d, genes_dict =prepare_dictionary_index(sim)\n",
        "    x = h5py.File(sim)\n",
        "    #ribosome state\n",
        "    active = x.get(\"states/Ribosome/nActive/data\")\n",
        "    stalled = x.get(\"states/Ribosome/nStalled/data\")\n",
        "    nonexist = x.get(\"states/Ribosome/nNotExist/data\")\n",
        "    #define the total number of ribosomes and take this info from the simulation avoiding the 0\n",
        "    tot_rib = active[0,0,frame]+nonexist[0,0,frame]+stalled[0,0,frame]\n",
        "    #-1=>stalled, 0=>not exist, 1=>actively translating\n",
        "    state = x.get(\"states/Ribosome/states/data\")\n",
        "    #gene that is being transcribed /  transcript bound to a ribosome /\n",
        "    #Karr: protein coding gene indices indicate the indices of the proteins in the array of proteins.\n",
        "    boundRNA = x.get(\"states/Ribosome/boundMRNAs/data\")\n",
        "    #mRNA position in codons of each 70s ribosome from the start codon. equal to the length of the polypeptide\n",
        "    mRNAPosition = x.get(\"states/Ribosome/mRNAPositions/data\")\n",
        "    #position of the ribosome (in codons) along the tmRNA template\n",
        "    tmRNAPosition = x.get(\"states/Ribosome/tmRNAPositions/data\")\n",
        "    #make an array of 4 columns with integers full of only 0\n",
        "    #a = np.zeros(boundRNA.shape[0],\n",
        "    #add a 15th column for the number of Ribosome bound\n",
        "    a=np.zeros(tot_rib,\n",
        "                 dtype = 'U20, i4, i4, U20, U20, i4, i4, U20, i4, i4, U20, U20, i4, i4, i4, i4, i4')\n",
        "                 #dtype =i4 for numbers, S20=strings\n",
        "    #update the array. 1st column state, 2nd column gene translated, 3rd column rib position on mRNA, 4th column rib\n",
        "    # 0 state\n",
        "    a['f1'] = np.array(state[:tot_rib,:,frame]).flatten()\n",
        "    # 1 boundmRNA -  prot index\n",
        "    a['f2'] = np.array(boundRNA[:tot_rib,:,frame]).flatten()\n",
        "    # 2 ribosome position in codons\n",
        "    a['f12'] = np.array(mRNAPosition[:tot_rib,:,frame]).flatten()\n",
        "    # 3 ribosome position in codons if ribosome stalled\n",
        "    a['f14'] = np.array(tmRNAPosition[:tot_rib,:,frame]).flatten()\n",
        "    for i, idx in enumerate(a['f2']):\n",
        "        a['f0'][i] = 'RIBOSOME'\n",
        "        try:\n",
        "            # 4 gene name\n",
        "            a['f4'][i] = d[idx][\"gene\"]\n",
        "            # 5 protein name dtype S20\n",
        "            a['f3'][i] = d[idx][\"protID\"]\n",
        "            # gene length\n",
        "            a['f6'][i] = d[idx]['gene_length']\n",
        "            # transcription unit name\n",
        "            a['f7'][i] = d[idx]['TU']\n",
        "            # length transcription unit\n",
        "            a['f8'][i] = d[idx]['length_TU']\n",
        "            # mono/polycistronic\n",
        "            a['f11'][i] = d[idx]['type']\n",
        "            # 11 direction\n",
        "            a['f10'][i] = d[idx]['direction']\n",
        "            # starting coord forward gene /end reverse genes\n",
        "            a['f5'][i] = d[idx]['coord_gene']\n",
        "            # starting coordinate TU\n",
        "            a['f9'][i] = d[idx]['coord_tu']\n",
        "        except:\n",
        "            #print (i,idx)\n",
        "            continue\n",
        "    #messing around in the next line for debugging\n",
        "    #a_sorted =a\n",
        "    a_sorted = np.sort(a, order=['f2'], axis=0)\n",
        "    #rinv is the mRNA id\n",
        "    unq,rinv, count = np.unique(a_sorted, axis=0,return_inverse=True, return_counts=True)\n",
        "    #makes another array with only the lines that are repeated > 1\n",
        "    repeated_groups = unq[count > 1]\n",
        "    #for each line that has duplicates make an array with the indices of those lines in a\n",
        "    for repeated_group in repeated_groups:\n",
        "        #this 'continue' is to make work ribosome_mRNA functions\n",
        "        #so it DOES NOT check for repeted postions in this moment\n",
        "        continue\n",
        "        repeated_idx = np.argwhere(a_sorted == repeated_group)\n",
        "        #print (repeated_idx)\n",
        "        for i in range(len(repeated_idx)):\n",
        "            n=20 #number of codons to shift the ribosome on the same gene. Produces a 60nt shift in ribosome position\n",
        "            if a_sorted['f1'][repeated_idx[i]]!=0: #if the state is not 'non existent'\n",
        "                #calculate the new ribosome position by adding 20*0, 20*1, 20*2...\n",
        "                #i is the index so n*20=0 => the position in the first repeated line will not change\n",
        "                new_position=a_sorted['f12'][repeated_idx[i]]+n*i\n",
        "                #calculated the ribosome positin in nt => we need this to check that the ribosome position is not longer than the gene\n",
        "                new_position_in_nt = new_position*3\n",
        "                #if the ribosome new ribosome position isnt longer than the gene\n",
        "                if new_position_in_nt<a_sorted['f6'][repeated_idx[i]]: #gene length\n",
        "                    a_sorted['f12'][repeated_idx[i]]=new_position\n",
        "                    #print ('new_position '+str(new_position))\n",
        "                #if the previous condition fail the ribosome position\n",
        "                else:\n",
        "                    print ('error ribosome shift')\n",
        "                    #shift back 20 codons\n",
        "                    a_sorted['f12'][repeated_idx[i]]=a_sorted['f12'][repeated_idx[i]]-n*i\n",
        "    #mrna id\n",
        "    a_sorted['f16'] = rinv#id\n",
        "    # 10 ribosome position in nt =  position in codons * 3\n",
        "    a_sorted['f13'] = a_sorted['f12']*3\n",
        "    #### RIBOSOME POSITION onto fulle length mRNA CALCULATIONS\n",
        "    #ribosome fw=  (coord gene -  coord tu) + rib position\n",
        "    i = np.where(a_sorted['f10'] == 'Forward')\n",
        "    #a['f14']==RIBOSOME POSITION\n",
        "    a_sorted['f15'][i] = (a_sorted['f5'][i] - a_sorted['f9'][i]) + a_sorted['f13'][i]\n",
        "    #ribosome rv = [(tu coord + tu length) - (gene coord + gene length)] +rib position\n",
        "    z = np.where(a_sorted['f10'] =='Reverse')\n",
        "    a_sorted['f15'][z] = ((a_sorted['f9'][z] + a_sorted['f8'][z]) - (a_sorted['f5'][z]+ a_sorted['f6'][z])) + a_sorted['f13'][z]\n",
        "    return a_sorted\n",
        "\n",
        "#(PART 2)\n",
        "def Ribosome_RNAs(sim,frame):\n",
        "    a = ribosome(sim, frame, output_folder)\n",
        "    #array removing non-existing ribosomes\n",
        "    non_zero_rib = a[a['f1']!=0]\n",
        "    #f0='RIBOSOME', f16= mRNAid, f6 = length Gene, f7= TU, f8= length TU,\n",
        "    #f14=tmRNA position - I USE IT ONLY BECAUSE IT IS A LIST OF 0 f15=ribosome position\n",
        "    bound = non_zero_rib[['f0', 'f16', 'f6','f7', 'f8', 'f15', 'f14']]\n",
        "    for i in bound:\n",
        "        #if the ribosome is too close to the beginning of the transcript OR it was in postion 0\n",
        "        if i[5]<30:\n",
        "            idx = np.argwhere(bound == i)\n",
        "            print ('hey here ribosome position was too close to the beginning, 30nt shift applied', idx, i[5])\n",
        "            #add 30nt\n",
        "            bound['f15'][idx[0][0]]=bound['f15'][idx[0][0]]+30\n",
        "        #if the position + 30 is longer than the actual TU\n",
        "        if i[5]+30 > i[4]:\n",
        "            difference=(i[5]+30)-i[4]\n",
        "            idx = np.argwhere(bound == i)\n",
        "            #print (i[5]+30, i[4], difference)\n",
        "            bound['f15'][idx[0][0]]=bound['f15'][idx[0][0]]- difference\n",
        "            print ('hey here ribosome position was too close to the end of the mRNA, shifted back', idx, i[5])\n",
        "        #replace f14 (column of 0s) with a copy of the ribosome positions\n",
        "    bound['f14']=non_zero_rib['f15']\n",
        "    nRIB= len(bound)\n",
        "    #ARRAY FOR all  MRNAs\n",
        "    tRNA, rRNA, sRNA, mRNA = free_rna_count(sim, frame)\n",
        "    mRNA['f1'] = [n[:6] for n in mRNA['f1']]\n",
        "    #array that repeats the TU whose copy number is >1, number of repetitions is in f2=count\n",
        "    rep_mRNA = np.repeat(mRNA,mRNA['f2'])\n",
        "    nRNA= len(rep_mRNA)\n",
        "    #label=mRNA / mRNAid  / TU / mRNA length / ribosome position\n",
        "    tot_mRNA = np.zeros((nRIB+nRNA,), dtype = 'U20, i4, U20, i4, i4, i4')#, i4')\n",
        "    indices = np.arange(0,nRNA)\n",
        "    tot_mRNA['f0'][:nRNA]= rep_mRNA['f4'] #label: 'mRNA'\n",
        "    tot_mRNA['f1'][:nRNA]= indices\n",
        "    tot_mRNA['f2'][:nRNA]= rep_mRNA['f1'] #TU\n",
        "    tot_mRNA['f3'][:nRNA]= rep_mRNA['f3'] #mRNA length\n",
        "    tot_mRNA['f4'][:nRNA]= rep_mRNA['f2'] #count --- this is temporary\n",
        "    mRNA_lastpos={}\n",
        "    #RIBOSOMES\n",
        "    for i in range(nRIB):\n",
        "        #i=ribosome id\n",
        "        tu = bound['f7'][i]\n",
        "        mRNA_ids = np.where(tot_mRNA['f2'] == tu)[0]\n",
        "        #pick one randomly --------------------------NB this means it can change\n",
        "        mRNA_id = np.random.choice(mRNA_ids,1)[0]\n",
        "        bound['f16'][i] = mRNA_id\n",
        "        bound['f6'][i]=i\n",
        "        #bound = np.sort(bound, order=['f16'], axis=0)\n",
        "        #mRNA_lastpos[mRNA_id] = bound['f14'][i]\n",
        "        if (mRNA_id in mRNA_lastpos) :\n",
        "            #calculate the difference\n",
        "            diff = bound['f15'][i] - mRNA_lastpos[mRNA_id]  #0 same, 60 expected different\n",
        "            #print (i, diff)\n",
        "            if abs(diff) <= 60 : #if the difference is <60\n",
        "                #if shifting the ribosome is still (f15 ribosome position)< than gene length (f8) -30\n",
        "                if (mRNA_lastpos[mRNA_id] + 60)<(bound['f8'][i]-30):\n",
        "                    bound['f14'][i] = mRNA_lastpos[mRNA_id] + 60\n",
        "                    #rint (i, mRNA_id, bound['f15'][i], mRNA_lastpos[mRNA_id], mRNA_lastpos[mRNA_id] + 60)\n",
        "                else: #if shifting the ribosome over the gene length (f8)\n",
        "                    print ('error ribosome shift', i, bound['f7'][i] , bound['f15'][i])\n",
        "                   #new position for the ribosome at least 60nt before the end of the mRNA\n",
        "                    bound['f14'][i] = mRNA_lastpos[mRNA_id] - 60\n",
        "                    #print (mRNA_lastpos[mRNA_id] - 60)\n",
        "            else:#if the difference is > or = 60 keep the position\n",
        "                bound['f14'][i] = bound['f15'][i]\n",
        "        else :#if (mRNA_id NOT in mRNA_lastpos) :\n",
        "            bound['f14'][i] = bound['f15'][i]\n",
        "        mRNA_lastpos[mRNA_id] = bound['f14'][i] #last position on that given mRNA\n",
        "        #break\n",
        "    indices = np.arange(0,nRIB)\n",
        "    tot_mRNA['f0'][nRNA:]= bound['f0'] #label: 'RIBOSOME'\n",
        "    tot_mRNA['f1'][nRNA:]= indices\n",
        "    tot_mRNA['f2'][nRNA:]= bound['f7'] #TU\n",
        "    tot_mRNA['f3'][nRNA:]= bound['f8'] #TU length\n",
        "    tot_mRNA['f4'][nRNA:]= bound['f16'] #mRNA id\n",
        "    #tot_mRNA['f5'][nRNA:]= bound['f15'] #original position\n",
        "    tot_mRNA['f5'][nRNA:]= bound ['f14'] #new position\n",
        "    np.savetxt(output_folder+'mRNA_ribosome_'+str(frame)+'.csv', tot_mRNA,\n",
        "                                  #'U20, i4, U20, i4, i4, i4'\n",
        "               delimiter=\",\",  fmt='%s,%d,%s,%d,%d, %d',\n",
        "               header='#name, id, TU, TU length, copy id, ribosome_position (in RIBOSOME)')\n",
        "#CELL SIZE\n",
        "def cell_size(sim, frame):\n",
        "    f = h5py.File(sim)\n",
        "    #shape of states/Geometry\n",
        "    data_geom = f.get('states/Geometry/volume/data')\n",
        "    #volume unit is Liters\n",
        "    volume_liters = data_geom[0, 0, frame]\n",
        "    #convert volume from liters (L) to cubic meters (m3)\n",
        "    volume_m3 = volume_liters/1000\n",
        "    #calculate radius of the hypotetic sphere with this volume\n",
        "    r = ((3 * volume_m3) / (4 * np.pi))**(1/3)\n",
        "    print ('volume (m3)=', volume_m3)\n",
        "    print ('sphere radius (m)=', r)\n",
        "\n",
        "#HOW MUCH CHROMOSOME HAS BEEN BOUND BY RNAPOLY\n",
        "   #caclulates how much chromosome is explored by RNApolymerase\n",
        "   #RNA poly footprint is 75nt and it binds ssDNA and the region is dsDNA\n",
        "   #this function returns a list (a set actually) of nt that are touched by RNApoly up to a certain frame\n",
        "   #It assumes that RNA poly moves always in direction forward\n",
        "   #TAKES A LOT to complete\n",
        "\n",
        "def rnapoly_exploration(frame, sim):\n",
        "    x = h5py.File(sim)\n",
        "    pos_data = x.get(\"states/RNAPolymerase/positionStrands/data\")\n",
        "    #pos_data.shape --> (156, 2, 29649)\n",
        "    l1 =[]\n",
        "    l2=[]\n",
        "    #like saying for i in range 156: per ogni RNApoly\n",
        "    for i in range(len(pos_data)):\n",
        "        a=pos_data[i,0,:frame]\n",
        "        b=pos_data[i,1,:frame]\n",
        "        # a= array([ 17928, 266088,  84028, 196991,  82073])\n",
        "        # b= array([2, 1, 2, 1, 2])\n",
        "        idx1 = np.where(b == 1)\n",
        "        idx2 = np.where(b == 2)\n",
        "        #print(a[idx])\n",
        "        for z in a[idx1]:\n",
        "            if z!=0:\n",
        "                #footprtnt +75 (strand 1 directionality 5>3)\n",
        "                l1.append(z+range(75))\n",
        "        for z in a[idx2]:\n",
        "            if z!=0:\n",
        "                #footprint -75 (strand 1 directionality 3>5)\n",
        "                l2.append(z+range(75))\n",
        "        #to array\n",
        "        array1=np.array(l1).ravel()\n",
        "        array2=np.array(l2).ravel()\n",
        "        #to list\n",
        "        all_positions1 = list(array1)\n",
        "        all_positions2 = list(array2)\n",
        "        #make a set to remove redundancy\n",
        "        #break\n",
        "    set_positions1 = set(all_positions1)\n",
        "    set_positions2 = set(all_positions2)\n",
        "    set_positions_sum = set_positions1.union(set_positions2)\n",
        "        #break#\n",
        "    percent_explored_strand1=len(set_positions1)*100/580076\n",
        "    percent_explored_strand2=len(set_positions2)*100/580076\n",
        "    percent_explored_sum=len(set_positions_sum)*100/580076\n",
        "    print ('how many positions have been explored up to this frame:', len(set_positions_sum))\n",
        "    print ('percentage of explored chromosome up to this frame (%):', percent_explored_sum )\n",
        "    return set_positions_sum\n",
        "\n",
        "#list_frames=[145,1189,6960] corresponding to 149, 1184, 6973\n",
        "\n",
        "#usage\n",
        "#rnapoly_expl_f149 = rnapoly_exploration(145, sim)\n",
        "#rnapoly_expl_f1184 = rnapoly_exploration(1189, sim)\n",
        "#rnapoly_expl_f6973 = rnapoly_exploration(6960, sim)\n",
        "\n",
        "#make a dictionary ,then a json.\n",
        "def rnapoly_exploration_dic(dic, json_fname):\n",
        "    #basically a list of indexes\n",
        "    list_keys=[]\n",
        "    for i in range(len(dic)):\n",
        "        list_keys.append(i)\n",
        "    list_key = set(list_keys)\n",
        "    #the set coming from rnapoly_exploration has numpy.int32, that do not go on json, so we convert in float\n",
        "    frames_float =[]\n",
        "    for i in dic:\n",
        "        frames_float.append(float(i))\n",
        "    frames_float =  set(frames_float)\n",
        "    dictionary_rnapoly_frames =dict(zip(list_keys, frames_float))\n",
        "    #Dic2json(dictionary_rnapoly_frames, json_fname, output_folder)\n",
        "    return dictionary_rnapoly_frames\n",
        "\n",
        "#################### FUNCTIONS FOR RECIPE BUILDER\n",
        "\n",
        "#function to update the chain: if a specific pdb file has been assigned with a chain, only that PDB FILE with that specific name will update\n",
        "def updateChain(dic1, dic2):\n",
        "    for i in dic1:\n",
        "        f = dic1[i]['pdb_model']\n",
        "        if i in dic2:\n",
        "  #     update dictionary if the same model is found in dic2\n",
        "            if f in dic2[i]['pdb_model']:\n",
        "                dic1[i].update({'chain':dic2[i]['chain'], 'bu':dic2[i]['bu']})\n",
        "\n",
        "def updateChainMembrane_new(dic1, dic2):\n",
        "    for i in dic1:\n",
        "        f= dic1[i]['pdb_model']\n",
        "        if i in dic2:\n",
        "            for el in range(len(dic2[i])):\n",
        "                if f == dic2[i][el]['pdb_model']:\n",
        "                    dic1[i].update({'chain':dic2[i][el]['chain'], 'bu':dic2[i][el]['bu'], 'offset':dic2[i][el]['offset'], 'pcpalAxis':dic2[i][el]['pcpalAxis']})\n",
        "\n",
        "#update the mapping dictionary with a template dictionary\n",
        "def updateTemplates(dic1, dic2):\n",
        "    for i in dic1:\n",
        "        if i in dic2:\n",
        "            dic1[i].update({'template':dic2[i]})\n",
        "\n",
        "#use mod_quality dictionary to update all_mapping dictionary with the quality information.\n",
        "#dic1 will be updated with info in dic2 if they share a key\n",
        "def updateQuality (dic1, dic2, akey):\n",
        "    for i in dic1:\n",
        "        if i in dic2:\n",
        "            dic1[i].update({'quality':dic2[i][akey]})\n",
        "\n",
        "def updateQualityComplexes (dic1, dic2):\n",
        "    for i in dic1:\n",
        "        pdb = dic1[i]['pdb_model']\n",
        "        if pdb in dic2:\n",
        "            dic1[i].update({'quality':dic2[pdb]['quality']})\n",
        "\n",
        "#NB IF A SPECIFIC PDB HAS BEEN POSITIONED AND THE POSITION IS REPORTED IN 'membrane' this script will overwrite the dictionary\n",
        "#woks using the pdb_models, not using the protId\n",
        "def updateOffset(dic1, dic2):\n",
        "    for i in dic1:\n",
        "        f = dic1[i]['pdb_model']\n",
        "        if i in dic2:\n",
        "            if f in dic2[i]['pdb_model']:\n",
        "                dic1[i].update({'offset':dic2[i]['offset'], 'pcpalAxis':dic2[i]['pcpalAxis']})\n",
        "\n",
        "#update dictionary with dictionary\n",
        "def updateDict(dic1, dic2):\n",
        "    for i in dic1:\n",
        "            dic1[i].update({'pdb_model':dic2[i]['pdb_model'], 'chain':dic2[i]['chain']})\n",
        "\n",
        "#in a dictionary find all the keys that are empty, make a list and delete them\n",
        "def RemoveEmptyKeys(adic):\n",
        "    l = []\n",
        "    for i in adic:\n",
        "        if len(adic[i])==0:\n",
        "            l.append(i)\n",
        "    for i in l:\n",
        "        if i in adic:\n",
        "            if len(adic[i])==0:\n",
        "                del adic[i]\n",
        "\n",
        "#write json file from a dictionary\n",
        "def Dic2json(adic, fname):\n",
        "    jsonf = fname+'.json'\n",
        "    f= open(output_dir+jsonf,\"w\")\n",
        "    f.write(json.dumps(adic))\n",
        "    f.close()\n",
        "\n",
        "#merge dictionaries\n",
        "def MergeDictionaries(dic1, dic2, dic3):\n",
        "    dic3 = dict(dic1)\n",
        "    dic3.update(dic2)\n",
        "\n",
        "#update dictionary general\n",
        "def UpdateDictionary(dic1, dic2):\n",
        "    for i in dic1:\n",
        "        if i in dic2:\n",
        "            dic1[i].update(dic2[i])\n",
        "\n",
        "#make numpy array with a specific simulation\n",
        "def set_np_arrays(sim):\n",
        "    f = h5py.File(sim)\n",
        "    #make the arrays for the protein complexes names\n",
        "    labels0_compl = f.get(\"states/ProteinComplex/counts/labels/0\")\n",
        "    array_names_compl = np.array(labels0_compl)\n",
        "    #make the arrays for the protein monomers names\n",
        "    labels0_mono= f.get(\"states/ProteinMonomer/counts/labels/0\")\n",
        "    array_names_mono = np.array(labels0_mono)\n",
        "    #data_mono shape is (frames, compartment, id )\n",
        "    #data_compl shape is (id, compartment, frame)\n",
        "    data_compl = f.get(\"states/ProteinComplex/counts/data\")\n",
        "    data_mono = f.get(\"states/ProteinMonomer/counts/data\")\n",
        "    return array_names_mono, array_names_compl, data_mono, data_compl\n",
        "\n",
        "def ds_ss_DNA_binding(json_data): #list with dsDNA binding protein, dictionary with all DNA binding (ss + ds)\n",
        "    covert = json.load(open(json_data, 'r'))\n",
        "    l =[] #list only with dsDNA binding\n",
        "    d={} #dictionary with all DNA binding proteins\n",
        "    for i in covert['data']:\n",
        "        if i['dna_footprint']!= None:\n",
        "            d[i['wid']]={'dna_footprint':i['dna_footprint']['length'],'dna_binding':i['dna_footprint']['binding']}\n",
        "            if i['dna_footprint']['binding']=='dsDNA':\n",
        "                l.append(i['wid'])\n",
        "    return l, d\n",
        "\n",
        "def edit_copy_numb_cluster_all_states(dic_copy_numb):\n",
        "    ##create a list with the process, folded etc proteins in varius states, sum their copy number to the mature ones and then remove them from the dictionary\n",
        "    ##the only proteins left will be RNA bound\n",
        "    readme=open(output_folder+'READ_ME_PROTEIN_INFO.txt', 'w')\n",
        "    to_remove =[]\n",
        "    for i in copy_numb:\n",
        "        if '-' in i:#this are the ingredients that have some other kind of states other than mature\n",
        "            if d3[i.split('-')[0]]['binds_dsdna']=='0':#not DNA binding\n",
        "                #for all proteins not binding DNA and not ribosome or poly\n",
        "                #inactive, prcessed, folded, bound etc\n",
        "                if i !='RIBOSOME_70S-bound' and i !='RNA_POLYMERASE-bound' and i !='RNA_POLYMERASE_HOLOENZYME-bound':\n",
        "                    #print (i)\n",
        "                    to_remove.append(i)\n",
        "                    #print (i, copy_numb[i], copy_numb[i.split('-')[0]])\n",
        "                    new_copy_numb = copy_numb[i]['count']+copy_numb[i.split('-')[0]]['count']\n",
        "                    #print ('sum this copy number to the mature one', i, copy_numb[i]['count'])\n",
        "                    readme.write(i.split('-')[0]+' copy number is the sum of the mature state and '+i+'\\n')\n",
        "                    copy_numb[i.split('-')[0]].update({'count':new_copy_numb})\n",
        "                else: #for ribosome and rna poly\n",
        "                    to_remove.append(i)\n",
        "                    #print ('this goes to lattice nucleoid', i, copy_numb[i]['count'])\n",
        "                    readme.write(i+' copy number is accounted in the lattice nucleoid '+i.split('-')[0]+'\\n')\n",
        "            elif d3[i.split('-')[0]]['binds_dsdna']=='1':\n",
        "                if '-bound' in i:#remove DNA bound count - it is in the lattice\n",
        "                    to_remove.append(i)\n",
        "                    #print ('this goes to lattice nucleoid', i, copy_numb[i]['count'])\n",
        "                    readme.write(i+' copy number is accounted in the lattice nucleoid '+i.split('-')[0]+'\\n')\n",
        "                else:\n",
        "                    #DNA binding protein not bound to the dna are considered cytoplasmic proteins when they are inactivated, processed etc\n",
        "                    new_copy_numb = copy_numb[i]['count']+copy_numb[i.split('-')[0]]['count']\n",
        "                    #print ('sum this copy number to the mature one', i, copy_numb[i]['count'])\n",
        "                    readme.write(i.split('-')[0]+' copy number is the sum of the mature state and '+i+'\\n')\n",
        "                    copy_numb[i.split('-')[0]].update({'count':new_copy_numb})\n",
        "                    to_remove.append(i)\n",
        "    readme.close()\n",
        "    #remove DNA bound and proteins in other states than mature\n",
        "    #the only protein left with the bound state are the bound to RNA, the structures of them will be the same as\n",
        "    for i in to_remove:\n",
        "        if i in copy_numb:\n",
        "            del copy_numb[i]\n",
        "    return copy_numb\n",
        "\n",
        "#proteins count at one frame\n",
        "def all_state_proteins_at_one_frame(frame_m, frame_c, json_data):\n",
        "    all_data = json.load(open(json_data, 'r'))\n",
        "    array_names_mono, array_names_compl, data_mono, data_compl, lprotein, lcomplex, l, adic = set_arrays(sim, json_data)\n",
        "    #tot monomers = sum_over_compartments ( nascent + mature + bound + ... )\n",
        "    tot_individual_proteins = np.sum(data_mono[frame_m,:,:],axis=0)\n",
        "    #tot_complexes = sum_over_compartments ( nascent + mature + bound + ... )\n",
        "    tot_complexes = np.sum(data_compl[:,:,frame_c],axis=1)\n",
        "    anb = np.zeros(len(all_data['data']))\n",
        "    for i in range(len(tot_individual_proteins)):\n",
        "        prot_name = array_names_mono[i].decode().split(\"-\")[0]\n",
        "        state= array_names_mono[i].decode().split(\"-\")[1]\n",
        "        #if state=='mature' or state=='bound':\n",
        "        #print (i, l.index(prot_name, tot_individual_proteins[i]))\n",
        "        indice = l.index(prot_name)\n",
        "        anb[indice] += tot_individual_proteins[i]\n",
        "    for i in range(len(tot_complexes)):\n",
        "        complex_name = array_names_compl[i].decode().split(\"-\")[0]\n",
        "        state= array_names_compl[i].decode().split(\"-\")[1]\n",
        "        #complex_count = tot_complexes[i]\n",
        "        #if state=='mature' or state=='bound':\n",
        "        #print (i, l.index(prot_name, tot_individual_proteins[i]))\n",
        "        indice = l.index(complex_name)\n",
        "        anb[indice] += tot_complexes[i]\n",
        "    adictionary={}\n",
        "    for i in range(len(l)):\n",
        "        adictionary[l[i]] = anb[i]\n",
        "    return adictionary\n",
        "\n",
        "#write a recipe in csv format\n",
        "def make_csv_recipe(dic,rna,fname):\n",
        "    dictionary2csv = open (output_folder+fname+'.csv', 'w')\n",
        "    dictionary2csv.write('name, molecular_weight, confidence (HHpred),pdb, selection, bu, uniprot, label, surface, compartment, comments, complexation, template, quality (modfold=monomers/voroMQA=complexes), offset, pcpalAxis, protein copy number, method, dsDNA-binding, function \\n')\n",
        "    list_homology = ['phyre', 'intfold', 'raptor', 'swiss', 'galaxy', 'itasser']\n",
        "    dic==d3\n",
        "    for aname in d3:\n",
        "            quality='-1'\n",
        "            #quality_modfold='-1'\n",
        "            #quality_voroMQA='-1\n",
        "            complexation='0.0' #default\n",
        "            membrane = ''#false\n",
        "            tree=fname+\".mge\"\n",
        "            uniprot = ''\n",
        "            mw =''\n",
        "            offset = '0,0,0'\n",
        "            axis = '0,0,1'\n",
        "            if d3[aname]['offset']!='':\n",
        "                offset = d3[aname]['offset']\n",
        "                axis = d3[aname]['pcpalAxis']\n",
        "             #if the quality was assinged take it from d3, otherwise use the standard '-1'\n",
        "             #for the monomers the quality is ModFOLD score\n",
        "             #for the complexes te quality is voroMQA\n",
        "            if d3[aname]['quality']!='':\n",
        "                quality = d3[aname]['quality']\n",
        "            if d3[aname]['mw']!=None:\n",
        "                mw = d3[aname]['mw']\n",
        "            #if the uniprot was assinged take it from d3, otherwise use the standard '-1'\n",
        "            if 'uniprot' in d3[aname]:\n",
        "                uniprot = d3[aname]['uniprot']\n",
        "            if d3[aname]['method']in list_homology:\n",
        "                data_source='Homology Modeling'\n",
        "            if d3[aname]['method']=='feig':\n",
        "                data_source='CYT-MG-model'\n",
        "            if d3[aname]['method']=='solved':\n",
        "                data_source='PDB (MG experimentally solved)'\n",
        "                #pdb=d3[aname]['pdb_model'].upper()\n",
        "            if d3[aname]['method']=='PDB-homolog':\n",
        "                data_source='PDB-homolog'\n",
        "                #pdb=d3[aname]['pdb_model'].upper()\n",
        "            if d3[aname]['method']=='PDB-homolog-edit':\n",
        "                data_source='PDB-homolog (manually assembled)'\n",
        "   #for monomers\n",
        "            if d3[aname]['model']=='ProteinMonomer':\n",
        "                #if d3[aname]['quality']!='':\n",
        "                #    quality_modfold= d3[aname]['quality']\n",
        "                # for 'real' monomers, aka monomers that do not participate in protein  complexes\n",
        "                if len(d3[aname]['participation_in_complexes'])==0:\n",
        "                    complexation='0.0' #monomers complexation = 0\n",
        "                    #place them in the membrane\n",
        "                    if d3[aname]['compartment']=='m' or d3[aname]['compartment']=='tm':\n",
        "                         membrane='TRUE'\n",
        "                    #place them in the extracell\n",
        "                    if d3[aname]['compartment']=='e':\n",
        "                        tree=fname\n",
        "   # for monomers in complexes\n",
        "            #place them in the membrane even when they are not complexed.\n",
        "                if len(d3[aname]['participation_in_complexes'])!=0:\n",
        "                    complexation = '0.5'\n",
        "                    if d3[aname]['compartment']=='m' or d3[aname]['compartment']=='tm':\n",
        "                         membrane='TRUE'#Membrane protein complexation occurs following insertion [PUB_0018]\n",
        "                         complexation = '0.5'\n",
        "    # for complexes\n",
        "            if d3[aname]['model']=='ProteinComplex':\n",
        "             #   if d3[aname]['quality']!='':\n",
        "             #       quality_voroMQA= d3[aname]['quality']\n",
        "                complexation='1.0'\n",
        "                #membrane placement\n",
        "                if d3[aname]['compartment']=='m' or d3[aname]['compartment']=='tm':\n",
        "                    membrane='TRUE'\n",
        "                #extracell placement\n",
        "                if d3[aname]['compartment']=='e':\n",
        "                    tree=fname\n",
        "            astr=aname+\",\"\n",
        "            astr+=str(mw)+','\n",
        "            astr+=str(d3[aname]['HHpred'])+','\n",
        "            astr+=str(d3[aname]['pdb_model'])+','\n",
        "            astr+=str(d3[aname]['chain'])+','\n",
        "            astr+=str(d3[aname]['bu'])+','\n",
        "            astr+=str(uniprot)+','\n",
        "            astr+=str(d3[aname]['function'])+','\n",
        "            astr+=membrane+','\n",
        "            astr+=tree+','\n",
        "            astr+=str(d3[aname]['compartment'])+','\n",
        "            astr+=complexation+','\n",
        "            astr+=str(d3[aname]['template'])+','\n",
        "            astr+=str(quality) +','\n",
        "            #astr+=str(quality_modfold) +','\n",
        "            #astr+=str(quality_voroMQA) +','\n",
        "            astr+='\"'+str(offset) +'\",'\n",
        "            astr+='\"'+str(axis) +'\",'\n",
        "            astr+=str(d3[aname]['count'])+','\n",
        "            astr+=str(data_source)+','\n",
        "            astr+=str(d3[aname]['binds_dsdna'])+','\n",
        "            astr+=str(d3[aname]['functional_category'])+'\\n'\n",
        "            dictionary2csv.write(astr)\n",
        "    #after all the proteins add the RNA structures from rna dictionary\n",
        "    for i in rna:\n",
        "        dictionary2csv.write(str(i)+','+\n",
        "                             ''+','+#mw\n",
        "                             str('-1')+','+#confidence\n",
        "                             str(rna[i]['pdb_model'])+','+\n",
        "                             str(rna[i]['chain'])+','+\n",
        "                             str(rna[i]['bu'])+','+\n",
        "                             ''+','+ #uniprot\n",
        "                             str(rna[i]['function'])+',,'+#surface\n",
        "                             str('root.mge')+','+#compartment\n",
        "                             str(rna[i]['compartment'])+#comments\n",
        "                             ',,,'+\n",
        "                             str('-1')+#quality\n",
        "                             ',,,'+#\n",
        "                             str(rna[i]['offset'])+','+\n",
        "                             str(rna[i]['pcpalAxis'])+','+\n",
        "                             str(rna[i]['count'])+','+\n",
        "                             str(rna[i]['method'])+','+\n",
        "                             str('0')+','+\n",
        "                             str(rna[i]['function_category'])+'\\n')\n",
        "    dictionary2csv.close()\n"
      ],
      "metadata": {
        "id": "vwat7kt5ePS8",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b63681-fa61-4c79-a28d-1e2d924a7b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WC-MG-CellPACK-functions.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import CellPACK Recipe Builder script for use\n",
        "\n",
        "%%writefile WC-MG-CellPACK-RecipeBuilder-short.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "@author: MM\n",
        "Created on Wed Jul 28 16:08:03 2021\n",
        "ASSEMBLES A MG DRAFT RECIPE COMPATIBLE WITH MESOSCOPE\n",
        "\"\"\"\n",
        "\n",
        "exec(open('WC-MG-CellPACK-functions.py').read())\n",
        "\n",
        "input_dir ='input_files'+os.sep\n",
        "output_dir = output_folder = 'scripts_output'+os.sep\n",
        "\n",
        "#0. Pick the method options : \"automatic\" or \"curated\"\n",
        "#1. make d3: final recipe as a dictionary named d3\n",
        "#    1.1 general info on ingredients\n",
        "#        essentiality\n",
        "#        DNA binding\n",
        "#        functional_category\n",
        "#        functional_category_clustered\n",
        "#    1.2 d1 (dictionary monomers) & d2 (dictionary complexes)\n",
        "#        d1 (monomers)\n",
        "#        d2 (complexes)\n",
        "#    1.3 d3: d1 + d2\n",
        "#3. COPY NUMBER - takes 1/2 min to complete\n",
        "#2. RNA (tRNA+rRNA_sRNA)\n",
        "#4. recipe as csv file\n",
        "\n",
        "# Dynamically load method from flag file\n",
        "try:\n",
        "    with open(\"method_flag.txt\", \"r\") as f:\n",
        "        use_method = f.read().strip()\n",
        "        print(f\"Using recipe method: {use_method}\")\n",
        "except:\n",
        "    use_method = \"curated\"\n",
        "    print(\"‚ö†Ô∏è Defaulting to 'curated' method (flag file not found)\")\n",
        "\n",
        "\n",
        "#'auto' is for when you want a recipe only with only homologous structures\n",
        "#no PDB files are generated since all the pdb models come from the PDB\n",
        "#'curated' is for a recipe manually curated with homology models and pdb models\n",
        "\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "## 1.1 : GENERAL INFO ON INGREDIENTS\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "#**********************************\n",
        "# uniprot\n",
        "#**********************************\n",
        "e = input_dir+'uniprot.csv'\n",
        "uniprot={}\n",
        "with open(e) as csvfile2:\n",
        "    spamreader2 = csv.reader(csvfile2)\n",
        "    headers = next(spamreader2)\n",
        "    for row in spamreader2:\n",
        "        aname = row[0]\n",
        "        mw = row[1]\n",
        "        uniprot_code =row[2]\n",
        "        uniprot[aname] = {'uniprot':uniprot_code}\n",
        "##print ('uniprot > uniprot monomers')\n",
        "#**********************************\n",
        "# HHscore\n",
        "#**********************************\n",
        "c =input_dir+'HHpred_scores.csv'\n",
        "hh = {}\n",
        "with open(c) as csvfile:\n",
        "    spamreader = csv.reader(csvfile)\n",
        "    headers = next(spamreader)\n",
        "    for row in spamreader:\n",
        "        aname = row[0]\n",
        "        hh_coverage = float(row[8])\n",
        "        HH=float(row[9])\n",
        "        hh[aname]={'HHpred':HH, 'HHcov':hh_coverage}\n",
        "#**********************************\n",
        "# essentiality =  which proteins come from essential genes\n",
        "#**********************************\n",
        "essentiality={}\n",
        "gene_data = json.load(open(input_dir+'genes_data.json', 'r'))\n",
        "covert = json.load(open(input_dir+'protein_data.json', 'r'))\n",
        "for ingredient in covert['data']:\n",
        "    #1=essential, 0=non essential\n",
        "    essentiality[ingredient['wid']]={'is_essential':''}\n",
        "    if 'gene' in ingredient:\n",
        "        #print (ingredient['wid'], ingredient['gene'])\n",
        "        for i in gene_data['data']:\n",
        "            if i['wid']==ingredient['gene']:\n",
        "                essentiality[ingredient['wid']].update({'is_essential':i['is_essential']['value']})\n",
        "#**********************************\n",
        "#dsDNA binding\n",
        "#**********************************\n",
        "# dictionary with  dsDNA binding proteins ==1, non binding dsDNA==0\n",
        "#NB. elongation factors and ribosome do not bind dsDNA\n",
        "DNAbind={}\n",
        "dsDNA_bp_list, ds_ss_DNA_bp_dictionary = ds_ss_DNA_binding(input_dir+'protein_data.json')\n",
        "for ingredient in covert['data']:\n",
        "    #all ingredient have 0 in DNA binding unless they are listed in dsDNA_bp_list\n",
        "    DNAbind[ingredient['wid']]={'binds_dsdna':'0'}\n",
        "    if ingredient['wid'] in dsDNA_bp_list:\n",
        "        DNAbind[ingredient['wid']].update({'binds_dsdna':'1'})\n",
        "#**********************************\n",
        "#functional_category\n",
        "#**********************************\n",
        "#total of 31 categories from CYT-MG model - doi: 10.1016/j.jmgm.2015.02.004\n",
        "functional_category={}\n",
        "f = input_dir+'ingredient_function.csv'\n",
        "with open(f) as csvfile:\n",
        "    spamreader = csv.reader(csvfile)\n",
        "    #headers = next(spamreader)\n",
        "    for row in spamreader:\n",
        "        aname = row[0]\n",
        "        functional_category[aname] = {'function':row[1].strip()}\n",
        "#print ('functional_category > functions identified in MG-CYT model')\n",
        "#**********************************\n",
        "#functional_category_clustered\n",
        "#**********************************\n",
        "##total of 12 categories\n",
        "#clustering MG-CYT function classification in 12 macro categories\n",
        "metabolism=['glycolysis', 'amino acid metabolism', 'lipid metabolism', 'cofactor metabolism', 'sugar metabolism',\n",
        "'nucleotide metabolism']\n",
        "protein_transport_singaling =['membrane transport', 'signaling']\n",
        "DNA_replication_maintenance =['DNA degradation', 'DNA recombination','DNA remodeling/stabilization','DNA repair','foreign DNA processing','replication']\n",
        "RNA_synthesis_maturation =['RNA remodeling','RNA degradation','RNA processing','aminoacyl tRNA synthetase']\n",
        "protein_folding_maturation=['ribosome biogenesis','protein folding','protein degradation',\n",
        "'post-translational processing',]\n",
        "cytokinesis_motility=[ 'cytoskeleton', 'cell division', 'cell adhesion']\n",
        "functional_category_cluster={}\n",
        "for i in functional_category:\n",
        "    functional_category_cluster[i]={'function':''}\n",
        "    if functional_category[i]['function'] in metabolism:#1\n",
        "        functional_category_cluster[i]['function']='metabolism'\n",
        "    if functional_category[i]['function'] in protein_transport_singaling:#2\n",
        "        functional_category_cluster[i]['function']='protein transport/singaling'\n",
        "    if functional_category[i]['function'] in DNA_replication_maintenance:#3\n",
        "        functional_category_cluster[i]['function']='DNA replication/maintenance'\n",
        "    if functional_category[i]['function'] in RNA_synthesis_maturation:#4\n",
        "        functional_category_cluster[i]['function']='RNA synthesis/maturation'\n",
        "    if functional_category[i]['function'] in protein_folding_maturation:#5\n",
        "        functional_category_cluster[i]['function']='protein folding/maturation'\n",
        "    if functional_category[i]['function'] in cytokinesis_motility:#6\n",
        "        functional_category_cluster[i]['function']='cytokinesis/motility'\n",
        "    if functional_category[i]['function']=='host cell interaction':#7\n",
        "        functional_category_cluster[i]['function']='host cell interaction'\n",
        "    if functional_category[i]['function']=='MG-specific':#8\n",
        "        functional_category_cluster[i]['function']='MG-specific'\n",
        "    if functional_category[i]['function']=='transcription':#9\n",
        "        functional_category_cluster[i]['function']='transcription'\n",
        "    if functional_category[i]['function']=='translation':#10\n",
        "        functional_category_cluster[i]['function']='translation'\n",
        "    if functional_category[i]['function']=='lipoprotein':#11\n",
        "        functional_category_cluster[i]['function']='lipoprotein'\n",
        "    if functional_category[i]['function']=='unknown':#12\n",
        "        functional_category_cluster[i]['function']='uncharacterized'\n",
        "\n",
        "#-###################################################################################\n",
        "# 2. d1 & d2\n",
        "#-###################################################################################\n",
        "#this file holds info about all the availabe structural models\n",
        "all_dict = json.load(open(input_dir+'all_dict.json', 'r'))\n",
        "\n",
        "#**********************************\n",
        "# d1 :  monomers\n",
        "#**********************************\n",
        "d1={}\n",
        "function_updated = json.load(open(input_dir+'updated_function_dictionary.json', 'r'))\n",
        "compartment_updated = json.load(open(input_dir+'compartment_updated.json', 'r'))\n",
        "wc_workbook = load_workbook(input_dir+'WholeCellData.xlsx') #open file\n",
        "mono_sheet = wc_workbook['S3M-Protein Monomers']\n",
        "for row in mono_sheet.iter_rows(min_row=2):#skip the header\n",
        "    for cell in row:\n",
        "        if cell.column_letter=='A':\n",
        "            protId= cell.value\n",
        "        if cell.column_letter=='F':\n",
        "            seq= cell.value\n",
        "        if cell.column_letter=='E':\n",
        "            leng= float(cell.value)\n",
        "        if cell.column_letter=='G':\n",
        "            mw= cell.value\n",
        "            d1[protId] = {'sequence':seq, 'length':leng, 'function': function_updated[protId]['function'], 'functional_category':functional_category_cluster[protId]['function'] , 'compartment':compartment_updated[protId]['compartment'], 'mw':mw}\n",
        "#complexation by data from covert\n",
        "protdata = json.load(open(input_dir+'protein_data.json', 'r'))\n",
        "#biosynthesis = {}\n",
        "biosythesis_participants = {}\n",
        "for i in  protdata['data']:\n",
        "    if i['model']=='ProteinMonomer':\n",
        "        participation = []\n",
        "        participation_empty = []\n",
        "        #when monomers are not involved in any complex write '0'\n",
        "        if len(i['protein_complex_biosythesis_participants'])== 0:\n",
        "            biosythesis_participants[i['wid']]={'participation_in_complexes': participation_empty, 'model':'ProteinMonomer'}\n",
        "        #when monomers are involved in complexes\n",
        "        if len(i['protein_complex_biosythesis_participants'])!= 0:\n",
        "            for c in range(len(i['protein_complex_biosythesis_participants'])):\n",
        "                participation.append(i['protein_complex_biosythesis_participants'][c]['protein_complexes'])\n",
        "                biosythesis_participants[i['wid']]={'participation_in_complexes': participation, 'model':'ProteinMonomer'}\n",
        "#print ('biosythesis_participants > participation to complex yes/no')\n",
        "UpdateDictionary(d1, hh)\n",
        "UpdateDictionary(d1, uniprot)\n",
        "UpdateDictionary(d1, biosythesis_participants)\n",
        "UpdateDictionary(d1, essentiality)\n",
        "UpdateDictionary(d1, DNAbind)\n",
        "#**********************************\n",
        "#d2 :  complexes\n",
        "#**********************************\n",
        "d2={}\n",
        "compl_sheet = wc_workbook['S3N-Macromolecular complexes']\n",
        "for row in compl_sheet.iter_rows(min_row=2):#skip the header\n",
        "    for cell in row:\n",
        "        if cell.column_letter=='A':\n",
        "            protId= cell.value\n",
        "        if cell.column_letter=='B':\n",
        "            name= cell.value\n",
        "        if cell.column_letter=='F':\n",
        "            mw= cell.value\n",
        "            d2[protId] = {'function':name,  'functional_category':functional_category_cluster[protId]['function'], 'compartment': compartment_updated[protId]['compartment'], 'mw':mw}#, 'offset':'', 'pcpalAxis':''}\n",
        "#dictionary for composition and stoichiometrty\n",
        "protdata = json.load(open(input_dir+'protein_data.json', 'r'))\n",
        "#dictionary with the components and stochiometries\n",
        "biosynthesis = {}\n",
        "#dictionary with the list of the other complexes in which single complexes are involved\n",
        "biosythesis_participants = {}\n",
        "for i in  protdata['data']:\n",
        "    if i['model']=='ProteinComplex':\n",
        "        #print 'ID: '+i['wid']\n",
        "        mol = []\n",
        "        coeff = []\n",
        "        # molecules that make the complex\n",
        "        for x in range(len(i['biosynthesis'])):\n",
        "            mol.append(i['biosynthesis'][x]['molecule'])\n",
        "            coeff.append(i['biosynthesis'][x]['coefficient'])\n",
        "            biosynthesis[i['wid']]={'molecules': mol, 'coefficient':coeff}\n",
        "        # other complexes in which a complex might be involved\n",
        "        participation = []\n",
        "        for c in range(len(i['protein_complex_biosythesis_participants'])):\n",
        "            if len(i['protein_complex_biosythesis_participants'][c]['protein_complexes'])== 0:\n",
        "                not_other_complex = []\n",
        "                biosythesis_participants[i['wid']]={'participation_in_complexes': not_other_complex, 'model':'ProteinComplex'}\n",
        "            else:\n",
        "                participation.append(i['protein_complex_biosythesis_participants'][c]['protein_complexes'])\n",
        "                biosythesis_participants[i['wid']]={'participation_in_complexes': participation, 'model':'ProteinComplex'}\n",
        "#merge biosynthesis and biosythesis_participants\n",
        "UpdateDictionary(biosynthesis, biosythesis_participants)\n",
        "#update d2\n",
        "UpdateDictionary(d2, biosynthesis)\n",
        "#print ('d2: + biosynthesis and components')\n",
        "# HHPRED complexes\n",
        "HHpred1 ={} #HHpred1 = complex with not only monomers as components >>> HHpred left empty\n",
        "HHpred2 ={} #HHpred2 = complex built by n monomers\n",
        "#HHpred1 = complex with not only monomers as components >>> HHpred left empty\n",
        "#HHpred2 = complex built by n monomers\n",
        "#HHpred3 = complex built witho other complexes >>> 1) update d2 with HHpred2, 2) calculate HHpred using info from d2-updated\n",
        "for aname in d2:\n",
        "    hh_x_coeff =[]\n",
        "    sum_coeff = []\n",
        "    #for every element in 'molecules'\n",
        "    for i in d2[aname]['molecules']:\n",
        "        #if element in 'molecule' does not end in 'MONOMER' (to include the 'molecules' that are made by other complexes\n",
        "        #if element in 'molecule' is not the name of the key\n",
        "        #if element in 'molecule' is longer that 8, to exclude 'H', 'MGrrnA16S', MG_0001 and stuff like this\n",
        "        if i.endswith('MONOMER')==False and i!=aname and len(i)>8:\n",
        "            #print aname, i\n",
        "            HHpred1[aname]={'HHpred':''}\n",
        "   #if element in 'molecule' has not the same attribute listed above, do this\n",
        "        else:\n",
        "    # if this element is in d1 (meaning that must be a MG_XXX_MONOMER'):\n",
        "            if i in d1:\n",
        "                #get HHscore monomer part of the complex from d1\n",
        "                HHsingle = float(d1[i]['HHpred'])\n",
        "                #get the position of a monomer in the list 'molecules' in d2\n",
        "                index_molecules = d2[aname]['molecules'].index(i)\n",
        "                #get the coefficient of the monomer searched above using its position in the list 'coefficient' in d2\n",
        "                index_coeff = float(d2[aname]['coefficient'][index_molecules].split('-')[1])\n",
        "                #multiply HHpred score with the coefficient\n",
        "                hh_coeff_single = index_coeff*HHsingle\n",
        "                #make a list with HHpred*coefficient for each monomer of a complex\n",
        "                hh_x_coeff.append(hh_coeff_single)\n",
        "                #make a list with all the coefficient for each monomer\n",
        "                sum_coeff.append(index_coeff)\n",
        "                #calculate the weighted average of HHpred scores\n",
        "                weighted_average = sum(hh_x_coeff)/sum(sum_coeff)\n",
        "        if len(hh_x_coeff) !=0:\n",
        "            HHpred2[aname]={'HHpred':weighted_average}\n",
        "#update HHpred2 with HHpred1 : leave empty for those complexes that are made by other complexes and not only monomers\n",
        "HHpred2.update(HHpred1)\n",
        "#update d2 with the HHpred data calculated so far\n",
        "UpdateDictionary(d2, HHpred2)\n",
        "#print ('d2: + HHpred2')\n",
        "HHpred3 ={}\n",
        "for aname in d2:\n",
        "    #quando HHpred e' vuoto:\n",
        "    # if aname=='RIBOSOME_30S':\n",
        "     if (d2[aname]['HHpred'] == ''):\n",
        "        hh_x_coeff_d1 = []\n",
        "        hh_x_coeff_d2 = []\n",
        "        a = []\n",
        "        c = []\n",
        "        HH_d1 = 0\n",
        "        index_molecules_d1 = 0\n",
        "        index_coeff_d1 =0\n",
        "        HH_d2 = 0\n",
        "        index_molecules_d2 = 0\n",
        "        index_coeff_d2 =0\n",
        "        #print aname+'<<<<<'\n",
        "    #per tutti gli elementi in molecules che\n",
        "        for i in d2[aname]['molecules']:\n",
        "            if i in d1 and i!=aname:\n",
        "                HH_d1 = float(d1[i]['HHpred'])\n",
        "                index_molecules_d1 = d2[aname]['molecules'].index(i)\n",
        "                index_coeff_d1 = float(d2[aname]['coefficient'][index_molecules_d1].split('-')[1])\n",
        "                hh_coeff_single_d1 = index_coeff_d1*HH_d1\n",
        "                a.append(hh_coeff_single_d1)\n",
        "                c.append(index_coeff_d1)\n",
        "                #print  i, hh_coeff_single_d1\n",
        "    #questo se i componenti di un complesso sono complessi\n",
        "            if i in d2 and i!=aname and d2[i]['HHpred']!='':\n",
        "                HH_d2 = float(d2[i]['HHpred'])\n",
        "                index_molecules_d2 = d2[aname]['molecules'].index(i)\n",
        "             #get the coefficient of the monomer searched above using its position in the list 'coefficient' in d2\n",
        "                index_coeff_d2 = float(d2[aname]['coefficient'][index_molecules_d2].split('-')[1])\n",
        "             #multiply HHpred score with the coefficient\n",
        "                hh_coeff_single_d2 = index_coeff_d2*HH_d2\n",
        "             #make a list with HHpred*coefficient for each monomer of a complex\n",
        "                a.append(hh_coeff_single_d2)\n",
        "                c.append(index_coeff_d2)\n",
        "                #print i, hh_coeff_single_d2\n",
        "            # this i is a pain so i custom made its search\n",
        "            if i=='MG_224_9MER_GTP':\n",
        "                i='MG_224_MONOMER'\n",
        "                HH_d1 = float(d1[i]['HHpred'])\n",
        "                hh_coeff_single_d1 = 1*HH_d1\n",
        "                a.append(hh_coeff_single_d1)\n",
        "                c.append(1)\n",
        "        #all the i that do not respect the previous criteria\n",
        "            else:\n",
        "                continue\n",
        "        #calculate the weighted average of HHpred scores\n",
        "        if sum(c)!=0:\n",
        "            weighted_average = sum(a)/sum(c)\n",
        "            #print weighted_average\n",
        "            HHpred3[aname]={'HHpred':weighted_average}\n",
        "        if sum(c)==0:\n",
        "            HHpred3[aname]={'HHpred':''}\n",
        "            #print aname\n",
        "        if aname=='RIBOSOME_30S':\n",
        "           # print HHpred3['RIBOSOME_30S_IF3']['HHpred']\n",
        "            hh_x_coeff_d1 = []\n",
        "            l = []\n",
        "            m = []\n",
        "            for i in d2[aname]['molecules']:\n",
        "                if i in d1 and i!=aname:\n",
        "                    HH_d1 = float(d1[i]['HHpred'])\n",
        "                    index_molecules_d1 = d2[aname]['molecules'].index(i)\n",
        "                    index_coeff_d1 = float(d2[aname]['coefficient'][index_molecules_d1].split('-')[1])\n",
        "                    hh_coeff_single_d1 = index_coeff_d1*HH_d1\n",
        "                    l.append(hh_coeff_single_d1)\n",
        "                    m.append(index_coeff_d1)\n",
        "                    weighted_average2 = (sum(l)+float(d1['MG_196_MONOMER']['HHpred']))/(21)\n",
        "                    HHpred3['RIBOSOME_30S_IF3']={'HHpred':weighted_average2}\n",
        "#updare for RIBOSOME 70, need a second round\n",
        "for aname in HHpred3:\n",
        "   if aname=='RIBOSOME_70S':#\n",
        "       HHpred3[aname].update({'HHpred':(HHpred3['RIBOSOME_30S']['HHpred']+HHpred3['RIBOSOME_50S']['HHpred'])/2})\n",
        "#print ('d2: + HHpred3')\n",
        "UpdateDictionary(d2, HHpred3)\n",
        "UpdateDictionary(d1, essentiality)\n",
        "UpdateDictionary(d2, DNAbind)\n",
        "#-###################################################################################\n",
        "#d3\n",
        "#-###################################################################################\n",
        "#make d3\n",
        "monomer = d1\n",
        "multimer = d2\n",
        "#merge dic\n",
        "d3 = dict(monomer)\n",
        "d3.update(multimer)\n",
        "\n",
        "#assign 'method' in each ingredient of d3\n",
        "seq_file= input_dir+'method_selection.csv'\n",
        "with open(seq_file) as csvfile:\n",
        "    spamreader = csv.reader(csvfile)\n",
        "    headers = next(spamreader)\n",
        "    for row in spamreader:\n",
        "        aname = row[0]\n",
        "        method = row[1]\n",
        "        if use_method == 'curated' :\n",
        "            d3[aname].update({'method':method})\n",
        "        elif use_method == 'auto' :\n",
        "            d3[aname].update({'method':'PDB-homolog'})\n",
        "        else : #'curated'\n",
        "            d3[aname].update({'method':method})\n",
        "\n",
        "#update d3 with other dictionaries based on the method specified\n",
        "method = ['phyre', 'intfold', 'raptor', 'swiss', 'galaxy', 'itasser', 'PDB-homolog', 'PDB-homolog-edit', 'solved', 'feig','' ]\n",
        "for i in d3:\n",
        "    #print (i)\n",
        "    if d3[i]['method']==method[0]:\n",
        "        d3[i].update(all_dict[i][method[0]])\n",
        "    if d3[i]['method']==method[1]:\n",
        "        d3[i].update(all_dict[i][method[1]])\n",
        "    if d3[i]['method']==method[2]:\n",
        "        d3[i].update(all_dict[i][method[2]])\n",
        "    if d3[i]['method']==method[3]:\n",
        "        d3[i].update(all_dict[i][method[3]])\n",
        "    if d3[i]['method']==method[4]:\n",
        "        d3[i].update(all_dict[i][method[4]])\n",
        "    if d3[i]['method']==method[5]:\n",
        "        d3[i].update(all_dict[i][method[5]])\n",
        "    if d3[i]['method']==method[6]:\n",
        "        d3[i].update(all_dict[i][method[6]])\n",
        "    if d3[i]['method']==method[7]:\n",
        "        d3[i].update(all_dict[i][method[7]])\n",
        "    if d3[i]['method']==method[8]:\n",
        "        d3[i].update(all_dict[i][method[8]])\n",
        "    if d3[i]['method']==method[9]:\n",
        "        if len(all_dict[i][method[9]])==2:\n",
        "            d3[i].update(all_dict[i][method[9]]['1'])\n",
        "        else:\n",
        "            d3[i].update(all_dict[i][method[9]])\n",
        "    if d3[i]['method']==method[10]:\n",
        "        d3[i].update({'bu': '','chain': '', 'offset': '','pcpalAxis': '','pdb_model': '','quality': '', 'template':''})\n",
        "#jsonf = 'd3.json'\n",
        "#f= open(output_dir+jsonf,\"w\")\n",
        "#f.write(json.dumps(d3))\n",
        "#f.close()\n",
        "#print ('d3')\n",
        "\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "## 2 : COPY NUMBER\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "#@title Import CellPACK Recipe Builder Script for Use\n",
        "frame_mono = 149  #@param {type:\"integer\"}\n",
        "frame_complex = 150  #@param {type:\"integer\"}\n",
        "\n",
        "#set the array for  a specific simulation specified above 'sim'\n",
        "array_names_mono, array_names_compl, data_mono, data_compl = set_np_arrays(sim)\n",
        "l =[]\n",
        "for i in covert['data']:\n",
        "    l.append(i['wid'])\n",
        "states_c = []\n",
        "for i in array_names_compl:\n",
        "    states_c.append(i.decode().split('-')[1])\n",
        "states_c = set(states_c)\n",
        "#buond proteins count is defined in the nucleoid recipe so here we do not consider them\n",
        "states_m = []\n",
        "for i in array_names_mono:\n",
        "    states_m.append(i.decode().split('R-')[1])\n",
        "states_m = set(states_m)\n",
        "#buond proteins count is defined in the nucleoid recipe so here we do not consider them\n",
        "copy_numb = {}\n",
        "for i in l:\n",
        "    #initialize everything with 0\n",
        "    copy_numb[i]={'count':0, 'compartment':0}\n",
        "    for state in states_c:\n",
        "        aname = i+'-'+state\n",
        "        #complexes\n",
        "        if aname.encode() in array_names_compl:\n",
        "            position = np.where(array_names_compl == aname.encode())[0][0]\n",
        "            molecule_per_compartment = []\n",
        "            for index in range(6):\n",
        "                count=data_compl[position, index , frame_complex]\n",
        "                molecule_per_compartment.append(count)\n",
        "                #for all states that are inactive, processes etc take only the one >0\n",
        "                if count!=0 and state!='mature':\n",
        "                    #print (aname, index, count)\n",
        "                    #molecules in cytoplasm\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('0'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'c'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    #molecules in DNA compartment\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('1'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'d'}\n",
        "                        #print (aname)\n",
        "                    #molecules in extracellular space\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('2'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'e'}\n",
        "                        #print (aname)\n",
        "                    #molecules in membrane\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('3'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'m'}\n",
        "                        #print (aname)\n",
        "                    #molecules in transmembrane organelle cytoplasm\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('4'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'tc'}\n",
        "                        #print (aname)\n",
        "                    #molecules in transmembrane organelle membrane\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('5'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'tm'}\n",
        "                        #print (aname)\n",
        "                #mature proteins do not get the 'matrue' tag in the dictionary key\n",
        "                if state=='mature':\n",
        "                                    #molecules in cytoplasm\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('0'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'c'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    #molecules in DNA compartment\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('1'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'d'}\n",
        "                        #print (aname)\n",
        "                    #molecules in extracellular space\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('2'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'e'}\n",
        "                        #print (aname)\n",
        "                    #molecules in membrane\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('3'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'m'}\n",
        "                        #print (aname)\n",
        "                    #molecules in transmembrane organelle cytoplasm\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('4'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'tc'}\n",
        "                        #print (aname)\n",
        "                    #molecules in transmembrane organelle membrane\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('5'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'tm'}\n",
        "                        #print (aname)continue\n",
        "   #MONOMERS\n",
        "    for state in states_m:\n",
        "        aname = i+'-'+state\n",
        "        if aname.encode() in array_names_mono:\n",
        "            position = np.where(array_names_mono == aname.encode())[0][0]\n",
        "            molecule_per_compartment = []\n",
        "            for index in range(6):\n",
        "                count=data_mono[frame_mono, index , position]\n",
        "                molecule_per_compartment.append(count)\n",
        "                if count!=0 and state!='mature':\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('0'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'c'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('1'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'d'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('2'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'e'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('3'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'m'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('4'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'tc'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('5'):\n",
        "                        copy_numb[aname]={'count':max(molecule_per_compartment), 'compartment':'tm'}\n",
        "                        #print (aname, max(molecule_per_compartment))\n",
        "                if state=='mature':\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('0'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'c'}\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('1'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'d'}\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('2'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'e'}\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('3'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'m'}\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('4'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'tc'}\n",
        "                    if molecule_per_compartment.index(max(molecule_per_compartment))==float('5'):\n",
        "                        copy_numb[i]={'count':max(molecule_per_compartment), 'compartment':'tm'}\n",
        "#this sums all the copy number of the same ingredient in the same id\n",
        "#it DOES NOT count the DNA binding prot bound copy number to its mature one\n",
        "#the copy number here considers all protein states apart from DNA bindign protein 'bound'\n",
        "copy_numb = edit_copy_numb_cluster_all_states(copy_numb)\n",
        "UpdateDictionary(copy_numb, d3)\n",
        "for i in copy_numb:\n",
        "    if '-'  in i:\n",
        "        aname = i.split('-')[0]\n",
        "        copy_numb[i].update(d3[aname])\n",
        "\n",
        "d3=copy_numb\n",
        "#print ('d3 w copy number')\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "## 3 : RNA dictionary\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "\n",
        "#list of rna ingredients\n",
        "rna_list =[]\n",
        "rRNA_l = ['MGrrnA16S', 'MGrrnA23S', 'MGrrnA5S']\n",
        "tRNA_l = ['tRNA', 'tRNA-aminoacylated']#all tRNAs will be collapsed in two categories: mature, aminoacylated\n",
        "sRNA_l = ['MG_0001', 'MG_0002', 'MG_0003', 'MG_0004']\n",
        "rna_list= rRNA_l+tRNA_l+sRNA_l\n",
        "\n",
        "#count RNAs\n",
        "#TOTAL NUMBER OF RNAS not only free\n",
        "tRNA, rRNA, sRNA, mRNA = free_rna_count(sim, frame_complex)\n",
        "\n",
        "#sum of all tRNAs\n",
        "tRNA_aminmoacylated =[]\n",
        "tRNA_others =[] #mature, processed, damaged etc\n",
        "sum_tRNA=[]\n",
        "for i in tRNA:\n",
        "    if 'aminmoacylated' in i[1]:\n",
        "        tRNA_aminmoacylated.append(i[2])\n",
        "    else:\n",
        "        tRNA_others.append(i[2])\n",
        "sum_tRNA_aminmoacylated =sum(tRNA_aminmoacylated)\n",
        "sum_tRNA_others=sum(tRNA_others)\n",
        "sum_tRNA=[sum_tRNA_others,sum_tRNA_aminmoacylated]\n",
        "#sum_trna = sum(tRNA['f2']) #sum of all tRNAs\n",
        "\n",
        "#could be done faster but i suck\n",
        "MG_0001=[]\n",
        "MG_0002=[]\n",
        "MG_0003=[]\n",
        "MG_0004=[]\n",
        "for i in sRNA:\n",
        "    if 'MG_0001' in i[1]:\n",
        "        MG_0001.append(i[2])\n",
        "    if 'MG_0002' in i[1]:\n",
        "        MG_0002.append(i[2])\n",
        "    if 'MG_0003' in i[1]:\n",
        "        MG_0003.append(i[2])\n",
        "    if 'MG_0004' in i[1]:\n",
        "        MG_0004.append(i[2])\n",
        "sum_MG_0001=sum(MG_0001)\n",
        "sum_MG_0002=sum(MG_0002)\n",
        "sum_MG_0003=sum(MG_0003)\n",
        "sum_MG_0004=sum(MG_0004)\n",
        "sum_sRNA = [sum_MG_0001,sum_MG_0002, sum_MG_0003, sum_MG_0004]\n",
        "\n",
        "#sum of all rRNAs\n",
        "#sum_rrna = sum(rRNA['f2']) not sure why i did this before\n",
        "\n",
        "MGrrnA16S=[]\n",
        "MGrrnA23S=[]\n",
        "MGrrnA5S=[]\n",
        "for i in rRNA:\n",
        "    if 'MGrrnA16S' in i[1]:\n",
        "        MGrrnA16S.append(i[2])\n",
        "    if 'MGrrnA23S' in i[1]:\n",
        "        MGrrnA23S.append(i[2])\n",
        "    if 'MGrrnA5S' in i[1]:\n",
        "        MGrrnA5S.append(i[2])\n",
        "sum_MGrrnA16S=sum(MGrrnA16S)\n",
        "sum_MGrrnA23S=sum(MGrrnA23S)\n",
        "sum_MGrrnA5S=sum(MGrrnA5S)\n",
        "sum_rRNA = [sum_MGrrnA16S,sum_MGrrnA23S, sum_MGrrnA5S]\n",
        "\n",
        "#mRNA (for other purposes)\n",
        "sum_mRNA=[]\n",
        "for i in mRNA:\n",
        "    sum_mRNA.append(i[2])\n",
        "\n",
        "#first draft\n",
        "rna ={}\n",
        "for i in rna_list:\n",
        "    rna[i]={'function':'', 'compartment':'c', 'model': 'RNA',\n",
        "       'method': '', 'pdb_model': '', 'chain': '', 'bu': 'AU',\n",
        "       'offset': '', 'pcpalAxis': '', 'count':'', 'function_category':''}\n",
        "#rRNAs\n",
        "rRNA_chains = [' or :AA', ' or :BB', ' or :BA']\n",
        "for i in rRNA_l:\n",
        "    rna[i].update({'function': 'rRNA',\n",
        "       'method': 'PDB-homolog', 'pdb_model': '4V69', 'chain': rRNA_chains[rRNA_l.index(i)], 'bu': 'AU', 'count':sum_rRNA[rRNA_l.index(i)], 'function_category':'translation'})\n",
        "#tRNA\n",
        "for i in tRNA_l:\n",
        "    rna[i].update({'function': 'tRNA', 'method': 'PDB-homolog',\n",
        " 'pdb_model': '6TNA', 'chain':'', 'bu': 'AU', 'count':sum_tRNA[tRNA_l.index(i)], 'function_category':'translation'})\n",
        "#sRNAs\n",
        "sRNA_pdb=['4WFM', '4V2S', '3DHS', '2CZJ']\n",
        "sRNA_chiains=[' or :A', ' or :Q', ' or :A', ' or :B']\n",
        "sRNA_function=['RNA synthesis/maturation', 'RNA synthesis/maturation', 'RNA synthesis/maturation', 'translation']\n",
        "for i in sRNA_l:\n",
        "    rna[i].update({'function': 'sRNA',  'method': 'PDB-homolog',\n",
        " 'pdb_model': sRNA_pdb[sRNA_l.index(i)], 'chain':sRNA_chiains[sRNA_l.index(i)], 'bu': 'AU', 'count':sum_sRNA[sRNA_l.index(i)], 'function_category':sRNA_function[sRNA_l.index(i)]})\n",
        "\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "## 4: RECIPE IN CSV FORMAT\n",
        "## @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
        "output_csv_name = os.path.join(output_dir, f\"{use_method}_root.csv\")\n",
        "make_csv_recipe(d3, rna, use_method + '_root')\n",
        "\n"
      ],
      "metadata": {
        "id": "IPmb75PjqX00",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fcc221-762d-47f2-a1c1-e6236795d687"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WC-MG-CellPACK-RecipeBuilder-short.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import LatticeNucleoid Script for Use (Optional for 3D Model)\n",
        "\n",
        "%%writefile WC-MG-CellPACK-input_LatticeNucleoid.py\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Aug  5 01:06:01 2021\n",
        "@author: MM\n",
        "Get input data from WC-MG computational simulation for LatticeNucleoid\n",
        "\"\"\"\n",
        "exec(open('WC-MG-CellPACK-functions.py').read())\n",
        "import os\n",
        "input_dir ='input_files'+os.sep\n",
        "output_folder = 'scripts_output'+os.sep\n",
        "\n",
        "# Define frame mappings: (JSON frame = SIM frame + 1)\n",
        "frames_m = [150, 1185, 6974]\n",
        "frames_c = [146, 1190, 6961]  # JSON frame numbers\n",
        "sim_c_frames = [145, 1189, 6960]  # Corresponding simulation .h5 frame numbers\n",
        "\n",
        "# Run chromosome protein mapping for each snapshot\n",
        "for frame in frames_m:\n",
        "    chromosome_proteins(fname_m, frame, fname_c, frames_c[frames_m.index(frame)], sim)\n",
        "\n",
        "# Extract RNA polymerase, ribosome, and RNA state from simulation\n",
        "for i in sim_c_frames:\n",
        "    rna_poly(sim, i)\n",
        "    Ribosome_RNAs(sim, i)\n",
        "\n",
        "print(\"‚úÖ Lattice nucleoid input files saved in: scripts_output/\")"
      ],
      "metadata": {
        "id": "UgYdU0s7kJGr",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a04407-e5bb-4efb-c6f9-f19079ce8c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing WC-MG-CellPACK-input_LatticeNucleoid.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run CellPACK Input Preparation Scripts\n",
        "#@markdown Select which recipes to generate and whether to include LatticeNucleoid data:\n",
        "\n",
        "recipe_mode = \"both\"  #@param [\"curated\", \"auto\", \"both\"]\n",
        "include_lattice = True  #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "\n",
        "# Ensure output directory exists\n",
        "os.makedirs(\"scripts_output\", exist_ok=True)\n",
        "\n",
        "# Helper function to run recipe script with given method\n",
        "def run_recipe(method):\n",
        "  print(f\"\\nüîß Generating {method} recipe...\")\n",
        "  with open(\"method_flag.txt\", \"w\") as f:\n",
        "      f.write(method)\n",
        "  os.system(\"python WC-MG-CellPACK-RecipeBuilder-short.py\")\n",
        "  print(f\"‚úÖ {method.capitalize()} recipe saved to: scripts_output/{method}_root.csv\")\n",
        "\n",
        "# Run the selected recipe types\n",
        "if recipe_mode in [\"curated\", \"both\"]:\n",
        "    run_recipe(\"curated\")\n",
        "\n",
        "if recipe_mode in [\"auto\", \"both\"]:\n",
        "    run_recipe(\"auto\")\n",
        "\n",
        "# Optional: Run the LatticeNucleoid extraction script\n",
        "if include_lattice:\n",
        "  print(\"\\nüß¨ Generating LatticeNucleoid data...\")\n",
        "  os.system(\"python WC-MG-CellPACK-input_LatticeNucleoid.py\")\n",
        "  print(\"‚úÖ LatticeNucleoid input files saved to: scripts_output/\")\n"
      ],
      "metadata": {
        "id": "QDJT6N6DqpcJ",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a562474-9d75-46a4-cec8-789e1dc52c23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Generating curated recipe...\n",
            "‚úÖ Curated recipe saved to: scripts_output/curated_root.csv\n",
            "\n",
            "üîß Generating auto recipe...\n",
            "‚úÖ Auto recipe saved to: scripts_output/auto_root.csv\n",
            "\n",
            "üß¨ Generating LatticeNucleoid data...\n",
            "‚úÖ LatticeNucleoid input files saved to: scripts_output/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Upload the JSON Recipe to CellPack for the Prediction Model"
      ],
      "metadata": {
        "id": "T_dMjBZCpvXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title WORK IN PROGRESS\n",
        "\n",
        "# This section is reserved for features or steps that are currently under development.\n",
        "# It may include future automation for visualization, improved integration with\n",
        "# CellPACK-GPU, or enhanced metadata processing.\n",
        "# Not all functionality here is guaranteed to be stable or complete‚Äîuse with caution.\n",
        "\n"
      ],
      "metadata": {
        "id": "QBMIywdQz78k",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}